{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 05:00:06.717792: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-22 05:00:07.238098: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-22 05:00:07.238887: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-22 05:00:07.241102: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-22 05:00:07.484365: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, Activation, Add, Flatten\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Calls List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "syscalls = [\n",
    "\"sys_enter_llistxattr\",\n",
    "\"sys_enter_setgroups\",\n",
    "\"sys_enter_lremovexattr\",\n",
    "\"sys_enter_sethostname\",\n",
    "\"sys_enter_accept\",\n",
    "\"sys_enter_lseek\",\n",
    "\"sys_enter_setitimer\",\n",
    "\"sys_enter_accept4\",\n",
    "\"sys_enter_lsetxattr\",\n",
    "\"sys_enter_setns\",\n",
    "\"sys_enter_acct\",\n",
    "\"sys_enter_madvise\",\n",
    "\"sys_enter_setpgid\",\n",
    "\"sys_enter_add_key\",\n",
    "\"sys_enter_mbind\",\n",
    "\"sys_enter_setpriority\",\n",
    "\"sys_enter_adjtimex\",\n",
    "\"sys_enter_membarrier\",\n",
    "\"sys_enter_setregid\",\n",
    "\"sys_enter_personality\",\n",
    "\"sys_enter_memfd_create\",\n",
    "\"sys_enter_setresgid\",\n",
    "\"sys_enter_bind\",\n",
    "\"sys_enter_memfd_secret\",\n",
    "\"sys_enter_setresuid\",\n",
    "\"sys_enter_bpf\",\n",
    "\"sys_enter_migrate_pages\",\n",
    "\"sys_enter_setreuid\",\n",
    "\"sys_enter_brk\",\n",
    "\"sys_enter_mincore\",\n",
    "\"sys_enter_setrlimit\",\n",
    "\"sys_enter_capget\",\n",
    "\"sys_enter_mkdirat\",\n",
    "\"sys_enter_setsid\",\n",
    "\"sys_enter_capset\",\n",
    "\"sys_enter_mknodat\",\n",
    "\"sys_enter_setsockopt\",\n",
    "\"sys_enter_chdir\",\n",
    "\"sys_enter_mlock\",\n",
    "\"sys_enter_settimeofday\",\n",
    "\"sys_enter_chroot\",\n",
    "\"sys_enter_mlock2\",\n",
    "\"sys_enter_setuid\",\n",
    "\"sys_enter_clock_adjtime\",\n",
    "\"sys_enter_mlockall\",\n",
    "\"sys_enter_setxattr\",\n",
    "\"sys_enter_clock_getres\",\n",
    "\"sys_enter_mmap\",\n",
    "\"sys_enter_shmat\",\n",
    "\"sys_enter_clock_gettime\",\n",
    "\"sys_enter_mount\",\n",
    "\"sys_enter_shmctl\",\n",
    "\"sys_enter_clock_nanosleep\",\n",
    "\"sys_enter_mount_setattr\",\n",
    "\"sys_enter_shmdt\",\n",
    "\"sys_enter_clock_settime\",\n",
    "\"sys_enter_move_mount\",\n",
    "\"sys_enter_shmget\",\n",
    "\"sys_enter_clone\",\n",
    "\"sys_enter_move_pages\",\n",
    "\"sys_enter_shutdown\",\n",
    "\"sys_enter_clone3\",\n",
    "\"sys_enter_mprotect\",\n",
    "\"sys_enter_sigaltstack\",\n",
    "\"sys_enter_close\",\n",
    "\"sys_enter_mq_getsetattr\",\n",
    "\"sys_enter_signalfd4\",\n",
    "\"sys_enter_close_range\",\n",
    "\"sys_enter_mq_notify\",\n",
    "\"sys_enter_socket\",\n",
    "\"sys_enter_connect\",\n",
    "\"sys_enter_mq_open\",\n",
    "\"sys_enter_socketpair\",\n",
    "\"sys_enter_copy_file_range\",\n",
    "\"sys_enter_mq_timedreceive\",\n",
    "\"sys_enter_splice\",\n",
    "\"sys_enter_delete_module\",\n",
    "\"sys_enter_mq_timedsend\",\n",
    "\"sys_enter_statfs\",\n",
    "\"sys_enter_dup\",\n",
    "\"sys_enter_mq_unlink\",\n",
    "\"sys_enter_statx\",\n",
    "\"sys_enter_dup3\",\n",
    "\"sys_enter_mremap\",\n",
    "\"sys_enter_swapoff\",\n",
    "\"sys_enter_epoll_create1\",\n",
    "\"sys_enter_msgctl\",\n",
    "\"sys_enter_swapon\",\n",
    "\"sys_enter_epoll_ctl\",\n",
    "\"sys_enter_msgget\",\n",
    "\"sys_enter_symlinkat\",\n",
    "\"sys_enter_epoll_pwait\",\n",
    "\"sys_enter_msgrcv\",\n",
    "\"sys_enter_sync\",\n",
    "\"sys_enter_epoll_pwait2\",\n",
    "\"sys_enter_msgsnd\",\n",
    "\"sys_enter_sync_file_range\",\n",
    "\"sys_enter_eventfd2\",\n",
    "\"sys_enter_msync\",\n",
    "\"sys_enter_syncfs\",\n",
    "\"sys_enter_execve\",\n",
    "\"sys_enter_munlock\",\n",
    "\"sys_enter_sysinfo\",\n",
    "\"sys_enter_execveat\",\n",
    "\"sys_enter_munlockall\",\n",
    "\"sys_enter_syslog\",\n",
    "\"sys_enter_exit\",\n",
    "\"sys_enter_munmap\",\n",
    "\"sys_enter_tee\",\n",
    "\"sys_enter_exit_group\",\n",
    "\"sys_enter_name_to_handle_at\",\n",
    "\"sys_enter_tgkill\",\n",
    "\"sys_enter_faccessat\",\n",
    "\"sys_enter_nanosleep\",\n",
    "\"sys_enter_timer_create\",\n",
    "\"sys_enter_faccessat2\",\n",
    "\"sys_enter_newfstat\",\n",
    "\"sys_enter_timer_delete\",\n",
    "\"sys_enter_fadvise64\",\n",
    "\"sys_enter_newfstatat\",\n",
    "\"sys_enter_timer_getoverrun\",\n",
    "\"sys_enter_fallocate\",\n",
    "\"sys_enter_newuname\",\n",
    "\"sys_enter_timer_gettime\",\n",
    "\"sys_enter_fanotify_init\",\n",
    "\"sys_enter_open_by_handle_at\",\n",
    "\"sys_enter_timer_settime\",\n",
    "\"sys_enter_fanotify_mark\",\n",
    "\"sys_enter_open_tree\",\n",
    "\"sys_enter_timerfd_create\",\n",
    "\"sys_enter_fchdir\",\n",
    "\"sys_enter_openat\",\n",
    "\"sys_enter_timerfd_gettime\",\n",
    "\"sys_enter_fchmod\",\n",
    "\"sys_enter_openat2\",\n",
    "\"sys_enter_timerfd_settime\",\n",
    "\"sys_enter_fchmodat\",\n",
    "\"sys_enter_perf_event_open\",\n",
    "\"sys_enter_times\",\n",
    "\"sys_enter_fchown\",\n",
    "\"sys_enter_pidfd_getfd\",\n",
    "\"sys_enter_tkill\",\n",
    "\"sys_enter_fchownat\",\n",
    "\"sys_enter_pidfd_open\",\n",
    "\"sys_enter_truncate\",\n",
    "\"sys_enter_fcntl\",\n",
    "\"sys_enter_pidfd_send_signal\",\n",
    "\"sys_enter_umask\",\n",
    "\"sys_enter_fdatasync\",\n",
    "\"sys_enter_pipe2\",\n",
    "\"sys_enter_umount\",\n",
    "\"sys_enter_fgetxattr\",\n",
    "\"sys_enter_pivot_root\",\n",
    "\"sys_enter_unlinkat\",\n",
    "\"sys_enter_finit_module\",\n",
    "\"sys_enter_ppoll\",\n",
    "\"sys_enter_unshare\",\n",
    "\"sys_enter_flistxattr\",\n",
    "\"sys_enter_prctl\",\n",
    "\"sys_enter_userfaultfd\",\n",
    "\"sys_enter_flock\",\n",
    "\"sys_enter_pread64\",\n",
    "\"sys_enter_utimensat\",\n",
    "\"sys_enter_fremovexattr\",\n",
    "\"sys_enter_preadv\",\n",
    "\"sys_enter_vhangup\",\n",
    "\"sys_enter_fsconfig\",\n",
    "\"sys_enter_preadv2\",\n",
    "\"sys_enter_vmsplice\",\n",
    "\"sys_enter_fsetxattr\",\n",
    "\"sys_enter_prlimit64\",\n",
    "\"sys_enter_wait4\",\n",
    "\"sys_enter_fsmount\",\n",
    "\"sys_enter_process_madvise\",\n",
    "\"sys_enter_waitid\",\n",
    "\"sys_enter_fsopen\",\n",
    "\"sys_enter_process_mrelease\",\n",
    "\"sys_enter_write\",\n",
    "\"sys_enter_fspick\",\n",
    "\"sys_enter_process_vm_readv\",\n",
    "\"sys_enter_writev\",\n",
    "\"sys_enter_fstatfs\",\n",
    "\"sys_enter_process_vm_writev\",\n",
    "\"sys_enter_fsync\",\n",
    "\"sys_enter_pselect6\",\n",
    "\"sys_enter_ftruncate\",\n",
    "\"sys_enter_ptrace\",\n",
    "\"sys_enter_futex\",\n",
    "\"sys_enter_pwrite64\",\n",
    "\"sys_enter_get_mempolicy\",\n",
    "\"sys_enter_pwritev\",\n",
    "\"sys_enter_get_robust_list\",\n",
    "\"sys_enter_pwritev2\",\n",
    "\"sys_enter_getcpu\",\n",
    "\"sys_enter_quotactl\",\n",
    "\"sys_enter_getcwd\",\n",
    "\"sys_enter_quotactl_fd\",\n",
    "\"sys_enter_getdents64\",\n",
    "\"sys_enter_read\",\n",
    "\"sys_enter_getegid\",\n",
    "\"sys_enter_readahead\",\n",
    "\"sys_enter_geteuid\",\n",
    "\"sys_enter_readlinkat\",\n",
    "\"sys_enter_getgid\",\n",
    "\"sys_enter_readv\",\n",
    "\"sys_enter_getgroups\",\n",
    "\"sys_enter_reboot\",\n",
    "\"sys_enter_getitimer\",\n",
    "\"sys_enter_recvfrom\",\n",
    "\"sys_enter_getpeername\",\n",
    "\"sys_enter_recvmmsg\",\n",
    "\"sys_enter_getpgid\",\n",
    "\"sys_enter_recvmsg\",\n",
    "\"sys_enter_getpid\",\n",
    "\"sys_enter_remap_file_pages\",\n",
    "\"sys_enter_getppid\",\n",
    "\"sys_enter_removexattr\",\n",
    "\"sys_enter_getpriority\",\n",
    "\"sys_enter_renameat\",\n",
    "\"sys_enter_getrandom\",\n",
    "\"sys_enter_renameat2\",\n",
    "\"sys_enter_getresgid\",\n",
    "\"sys_enter_request_key\",\n",
    "\"sys_enter_getresuid\",\n",
    "\"sys_enter_restart_syscall\",\n",
    "\"sys_enter_getrlimit\",\n",
    "\"sys_enter_rseq\",\n",
    "\"sys_enter_getrusage\",\n",
    "\"sys_enter_rt_sigaction\",\n",
    "\"sys_enter_getsid\",\n",
    "\"sys_enter_rt_sigpending\",\n",
    "\"sys_enter_getsockname\",\n",
    "\"sys_enter_rt_sigprocmask\",\n",
    "\"sys_enter_getsockopt\",\n",
    "\"sys_enter_rt_sigqueueinfo\",\n",
    "\"sys_enter_gettid\",\n",
    "\"sys_enter_rt_sigreturn\",\n",
    "\"sys_enter_gettimeofday\",\n",
    "\"sys_enter_rt_sigsuspend\",\n",
    "\"sys_enter_getuid\",\n",
    "\"sys_enter_rt_sigtimedwait\",\n",
    "\"sys_enter_getxattr\",\n",
    "\"sys_enter_rt_tgsigqueueinfo\",\n",
    "\"sys_enter_init_module\",\n",
    "\"sys_enter_sched_get_priority_max\",\n",
    "\"sys_enter_inotify_add_watch\",\n",
    "\"sys_enter_sched_get_priority_min\",\n",
    "\"sys_enter_inotify_init1\",\n",
    "\"sys_enter_sched_getaffinity\",\n",
    "\"sys_enter_inotify_rm_watch\",\n",
    "\"sys_enter_sched_getattr\",\n",
    "\"sys_enter_io_cancel\",\n",
    "\"sys_enter_sched_getparam\",\n",
    "\"sys_enter_io_destroy\",\n",
    "\"sys_enter_sched_getscheduler\",\n",
    "\"sys_enter_io_getevents\",\n",
    "\"sys_enter_sched_rr_get_interval\",\n",
    "\"sys_enter_io_pgetevents\",\n",
    "\"sys_enter_sched_setaffinity\",\n",
    "\"sys_enter_io_setup\",\n",
    "\"sys_enter_sched_setattr\",\n",
    "\"sys_enter_io_submit\",\n",
    "\"sys_enter_sched_setparam\",\n",
    "\"sys_enter_io_uring_enter\",\n",
    "\"sys_enter_sched_setscheduler\",\n",
    "\"sys_enter_io_uring_register\",\n",
    "\"sys_enter_sched_yield\",\n",
    "\"sys_enter_io_uring_setup\",\n",
    "\"sys_enter_seccomp\",\n",
    "\"sys_enter_ioctl\",\n",
    "\"sys_enter_semctl\",\n",
    "\"sys_enter_ioprio_get\",\n",
    "\"sys_enter_semget\",\n",
    "\"sys_enter_ioprio_set\",\n",
    "\"sys_enter_semop\",\n",
    "\"sys_enter_kcmp\",\n",
    "\"sys_enter_semtimedop\",\n",
    "\"sys_enter_kexec_file_load\",\n",
    "\"sys_enter_sendfile64\",\n",
    "\"sys_enter_kexec_load\",\n",
    "\"sys_enter_sendmmsg\",\n",
    "\"sys_enter_keyctl\",\n",
    "\"sys_enter_sendmsg\",\n",
    "\"sys_enter_kill\",\n",
    "\"sys_enter_sendto\",\n",
    "\"sys_enter_landlock_add_rule\",\n",
    "\"sys_enter_set_mempolicy\",\n",
    "\"sys_enter_landlock_create_ruleset\",\n",
    "\"sys_enter_set_robust_list\",\n",
    "\"sys_enter_landlock_restrict_self\",\n",
    "\"sys_enter_set_tid_address\",\n",
    "\"sys_enter_lgetxattr\",\n",
    "\"sys_enter_setdomainname\",\n",
    "\"sys_enter_linkat\",\n",
    "\"sys_enter_setfsgid\",\n",
    "\"sys_enter_listen\",\n",
    "\"sys_enter_setfsuid\",\n",
    "\"sys_enter_listxattr\",\n",
    "\"sys_enter_setgid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV from Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "CLASSES = np.array(['benign', 'sysrv', 'xmrig'])\n",
    "DATASET_DIR = \"raw_data\"\n",
    "VECTOR_LENGTH = 32 * 32\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(syscalls)\n",
    "\n",
    "def csvToVector(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    data_encoded = label_encoder.fit_transform(data['SYSTEM_CALL'])\n",
    "    vector = np.zeros(VECTOR_LENGTH, dtype=np.uint8)\n",
    "    syscall_nums = min(len(data_encoded), VECTOR_LENGTH)\n",
    "    vector[:syscall_nums] = data_encoded[:syscall_nums]\n",
    "\n",
    "    return vector\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, class_idx = args\n",
    "    vector = csvToVector(file_path)\n",
    "    return vector, class_idx\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    classes = [\"0/30sec_0\", \"1/30sec_1\", \"2/30sec_2\"]\n",
    "\n",
    "    file_paths = []\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                file_paths.append((file_path, class_idx))\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_file, file_paths)\n",
    "\n",
    "    x, y = zip(*results)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Split and Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 299.0\n",
    "X_val = X_val / 299.0\n",
    "X_test = X_test / 299.0\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2072, 1024)\n",
      "(1110, 1024)\n",
      "(2072, 3)\n",
      "(1110, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 05:01:29.753762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(VECTOR_LENGTH, 1))\n",
    "\n",
    "x = Conv1D(filters=32, kernel_size=3, padding='same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=64, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1024, 32)          128       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024, 32)          128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1024, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 512, 32)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 512, 64)           6208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512, 64)           256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 512, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 256, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 256, 128)          24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 256, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 256, 128)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 128, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4194560   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4228291 (16.13 MB)\n",
      "Trainable params: 4227331 (16.13 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/tmp/30_CNN1d_checkpoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:42:10.006659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-07-29 17:42:11.412720: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0bda52ac00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 17:42:11.412756: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:42:11.412762: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:42:11.412767: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:42:11.418590: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 17:42:11.511878: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.8465\n",
      "Epoch 1: accuracy improved from -inf to 0.84653, saving model to /tmp/30_CNN1d_checkpoint.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 6s 17ms/step - loss: 0.5836 - accuracy: 0.8465 - val_loss: 1.3111 - val_accuracy: 0.1274 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9316\n",
      "Epoch 2: accuracy improved from 0.84653 to 0.93147, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 0.2267 - accuracy: 0.9315 - val_loss: 1.3560 - val_accuracy: 0.1274 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9492\n",
      "Epoch 3: accuracy improved from 0.93147 to 0.94981, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.1817 - accuracy: 0.9498 - val_loss: 1.4338 - val_accuracy: 0.1293 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.9619\n",
      "Epoch 4: accuracy improved from 0.94981 to 0.96187, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.1470 - accuracy: 0.9619 - val_loss: 1.4712 - val_accuracy: 0.3147 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9702\n",
      "Epoch 5: accuracy improved from 0.96187 to 0.96911, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.1290 - accuracy: 0.9691 - val_loss: 1.7179 - val_accuracy: 0.2008 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9531\n",
      "Epoch 6: accuracy did not improve from 0.96911\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1501 - accuracy: 0.9527 - val_loss: 1.0908 - val_accuracy: 0.3494 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9634\n",
      "Epoch 7: accuracy did not improve from 0.96911\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1282 - accuracy: 0.9638 - val_loss: 1.9007 - val_accuracy: 0.3205 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9678\n",
      "Epoch 8: accuracy did not improve from 0.96911\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1218 - accuracy: 0.9677 - val_loss: 0.9639 - val_accuracy: 0.2066 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1137 - accuracy: 0.9727\n",
      "Epoch 9: accuracy improved from 0.96911 to 0.97297, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 0.1133 - accuracy: 0.9730 - val_loss: 0.9617 - val_accuracy: 0.3127 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9697\n",
      "Epoch 10: accuracy did not improve from 0.97297\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1108 - accuracy: 0.9701 - val_loss: 1.5386 - val_accuracy: 0.6023 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9785\n",
      "Epoch 11: accuracy improved from 0.97297 to 0.97876, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0901 - accuracy: 0.9788 - val_loss: 8.8287 - val_accuracy: 0.2780 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9727\n",
      "Epoch 12: accuracy did not improve from 0.97876\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0954 - accuracy: 0.9730 - val_loss: 2.0734 - val_accuracy: 0.6274 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9751\n",
      "Epoch 13: accuracy did not improve from 0.97876\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0979 - accuracy: 0.9744 - val_loss: 6.1901 - val_accuracy: 0.2934 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9800\n",
      "Epoch 14: accuracy improved from 0.97876 to 0.97925, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0888 - accuracy: 0.9792 - val_loss: 0.6865 - val_accuracy: 0.8320 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9775\n",
      "Epoch 15: accuracy did not improve from 0.97925\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0936 - accuracy: 0.9773 - val_loss: 0.5907 - val_accuracy: 0.8552 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9800\n",
      "Epoch 16: accuracy improved from 0.97925 to 0.98021, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0844 - accuracy: 0.9802 - val_loss: 1.2870 - val_accuracy: 0.7162 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9771\n",
      "Epoch 17: accuracy did not improve from 0.98021\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0872 - accuracy: 0.9773 - val_loss: 3.9279 - val_accuracy: 0.6120 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1020 - accuracy: 0.9741\n",
      "Epoch 18: accuracy did not improve from 0.98021\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9744 - val_loss: 14.8377 - val_accuracy: 0.2780 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9712\n",
      "Epoch 19: accuracy did not improve from 0.98021\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1023 - accuracy: 0.9715 - val_loss: 5.0342 - val_accuracy: 0.7259 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.9766\n",
      "Epoch 20: accuracy did not improve from 0.98021\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0873 - accuracy: 0.9759 - val_loss: 3.5486 - val_accuracy: 0.7992 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9824\n",
      "Epoch 21: accuracy improved from 0.98021 to 0.98214, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0776 - accuracy: 0.9821 - val_loss: 7.3540 - val_accuracy: 0.2780 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0771 - accuracy: 0.9800\n",
      "Epoch 22: accuracy did not improve from 0.98214\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9797 - val_loss: 7.6855 - val_accuracy: 0.2819 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9854\n",
      "Epoch 23: accuracy improved from 0.98214 to 0.98456, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0706 - accuracy: 0.9846 - val_loss: 13.0149 - val_accuracy: 0.2780 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9834\n",
      "Epoch 24: accuracy did not improve from 0.98456\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9831 - val_loss: 1.0411 - val_accuracy: 0.8552 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9844\n",
      "Epoch 25: accuracy did not improve from 0.98456\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0683 - accuracy: 0.9841 - val_loss: 1.4816 - val_accuracy: 0.6699 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0673 - accuracy: 0.9829\n",
      "Epoch 26: accuracy did not improve from 0.98456\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0668 - accuracy: 0.9831 - val_loss: 0.6522 - val_accuracy: 0.6351 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9824\n",
      "Epoch 27: accuracy did not improve from 0.98456\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9826 - val_loss: 4.2149 - val_accuracy: 0.2915 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9849\n",
      "Epoch 28: accuracy improved from 0.98456 to 0.98504, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0726 - accuracy: 0.9850 - val_loss: 3.0826 - val_accuracy: 0.2896 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9824\n",
      "Epoch 29: accuracy did not improve from 0.98504\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0736 - accuracy: 0.9826 - val_loss: 0.1930 - val_accuracy: 0.9208 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9839\n",
      "Epoch 30: accuracy did not improve from 0.98504\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0685 - accuracy: 0.9836 - val_loss: 0.2613 - val_accuracy: 0.9170 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9844\n",
      "Epoch 31: accuracy did not improve from 0.98504\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 0.9846 - val_loss: 0.1511 - val_accuracy: 0.9633 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9868\n",
      "Epoch 32: accuracy improved from 0.98504 to 0.98649, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0635 - accuracy: 0.9865 - val_loss: 0.0805 - val_accuracy: 0.9807 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9844\n",
      "Epoch 33: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0633 - accuracy: 0.9846 - val_loss: 3.4990 - val_accuracy: 0.3301 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9854\n",
      "Epoch 34: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0665 - accuracy: 0.9850 - val_loss: 7.6012 - val_accuracy: 0.2857 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0656 - accuracy: 0.9844\n",
      "Epoch 35: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0667 - accuracy: 0.9841 - val_loss: 10.4548 - val_accuracy: 0.2819 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0671 - accuracy: 0.9849\n",
      "Epoch 36: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0664 - accuracy: 0.9850 - val_loss: 0.1800 - val_accuracy: 0.9633 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9849\n",
      "Epoch 37: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0637 - accuracy: 0.9850 - val_loss: 0.1823 - val_accuracy: 0.9208 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9839\n",
      "Epoch 38: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0675 - accuracy: 0.9836 - val_loss: 0.1693 - val_accuracy: 0.9459 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9849\n",
      "Epoch 39: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0581 - accuracy: 0.9850 - val_loss: 1.6575 - val_accuracy: 0.3533 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9849\n",
      "Epoch 40: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0640 - accuracy: 0.9850 - val_loss: 0.5505 - val_accuracy: 0.8166 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9854\n",
      "Epoch 41: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0618 - accuracy: 0.9855 - val_loss: 0.5943 - val_accuracy: 0.6332 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9839\n",
      "Epoch 42: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0648 - accuracy: 0.9841 - val_loss: 0.2671 - val_accuracy: 0.8320 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9858\n",
      "Epoch 43: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.9860 - val_loss: 0.2049 - val_accuracy: 0.9788 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9854\n",
      "Epoch 44: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.9855 - val_loss: 0.1767 - val_accuracy: 0.9730 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9849\n",
      "Epoch 45: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0619 - accuracy: 0.9846 - val_loss: 0.0670 - val_accuracy: 0.9846 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.9839\n",
      "Epoch 46: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0733 - accuracy: 0.9841 - val_loss: 0.0671 - val_accuracy: 0.9826 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9854\n",
      "Epoch 47: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0590 - accuracy: 0.9855 - val_loss: 0.1362 - val_accuracy: 0.9749 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "63/65 [============================>.] - ETA: 0s - loss: 0.0617 - accuracy: 0.9856\n",
      "Epoch 48: accuracy did not improve from 0.98649\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0611 - accuracy: 0.9855 - val_loss: 0.0826 - val_accuracy: 0.9788 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9878\n",
      "Epoch 49: accuracy improved from 0.98649 to 0.98745, saving model to /tmp/30_CNN1d_checkpoint.h5\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 0.0585 - accuracy: 0.9875 - val_loss: 0.0921 - val_accuracy: 0.9807 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9863\n",
      "Epoch 50: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0626 - accuracy: 0.9865 - val_loss: 0.1775 - val_accuracy: 0.9575 - lr: 6.2500e-05\n",
      "Epoch 51/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9863\n",
      "Epoch 51: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0615 - accuracy: 0.9860 - val_loss: 0.0872 - val_accuracy: 0.9807 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9863\n",
      "Epoch 52: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0589 - accuracy: 0.9865 - val_loss: 0.0616 - val_accuracy: 0.9865 - lr: 3.1250e-05\n",
      "Epoch 53/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9868\n",
      "Epoch 53: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0534 - accuracy: 0.9870 - val_loss: 0.0570 - val_accuracy: 0.9865 - lr: 3.1250e-05\n",
      "Epoch 54/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9863\n",
      "Epoch 54: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0573 - accuracy: 0.9865 - val_loss: 0.0627 - val_accuracy: 0.9807 - lr: 3.1250e-05\n",
      "Epoch 55/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9858\n",
      "Epoch 55: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0589 - accuracy: 0.9860 - val_loss: 0.0648 - val_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 56/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9863\n",
      "Epoch 56: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0580 - accuracy: 0.9865 - val_loss: 0.0803 - val_accuracy: 0.9807 - lr: 3.1250e-05\n",
      "Epoch 57/100\n",
      "63/65 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9846\n",
      "Epoch 57: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0550 - accuracy: 0.9850 - val_loss: 0.0666 - val_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 58/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0578 - accuracy: 0.9854\n",
      "Epoch 58: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0602 - accuracy: 0.9846 - val_loss: 0.0705 - val_accuracy: 0.9826 - lr: 3.1250e-05\n",
      "Epoch 59/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9858\n",
      "Epoch 59: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.9860 - val_loss: 0.0597 - val_accuracy: 0.9846 - lr: 1.5625e-05\n",
      "Epoch 60/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0569 - accuracy: 0.9863\n",
      "Epoch 60: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0581 - accuracy: 0.9860 - val_loss: 0.0635 - val_accuracy: 0.9826 - lr: 1.5625e-05\n",
      "Epoch 61/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9858\n",
      "Epoch 61: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0561 - accuracy: 0.9860 - val_loss: 0.0561 - val_accuracy: 0.9865 - lr: 1.5625e-05\n",
      "Epoch 62/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0521 - accuracy: 0.9873\n",
      "Epoch 62: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0556 - accuracy: 0.9865 - val_loss: 0.0620 - val_accuracy: 0.9826 - lr: 1.5625e-05\n",
      "Epoch 63/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9854\n",
      "Epoch 63: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0557 - accuracy: 0.9855 - val_loss: 0.0598 - val_accuracy: 0.9865 - lr: 1.5625e-05\n",
      "Epoch 64/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0577 - accuracy: 0.9868\n",
      "Epoch 64: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0571 - accuracy: 0.9870 - val_loss: 0.0587 - val_accuracy: 0.9846 - lr: 1.5625e-05\n",
      "Epoch 65/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0522 - accuracy: 0.9868\n",
      "Epoch 65: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0537 - accuracy: 0.9860 - val_loss: 0.0592 - val_accuracy: 0.9865 - lr: 1.5625e-05\n",
      "Epoch 66/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0624 - accuracy: 0.9858\n",
      "Epoch 66: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0618 - accuracy: 0.9860 - val_loss: 0.0659 - val_accuracy: 0.9826 - lr: 1.5625e-05\n",
      "Epoch 67/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9854\n",
      "Epoch 67: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0584 - accuracy: 0.9855 - val_loss: 0.0607 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9868\n",
      "Epoch 68: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0591 - accuracy: 0.9870 - val_loss: 0.0583 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0608 - accuracy: 0.9849\n",
      "Epoch 69: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0602 - accuracy: 0.9850 - val_loss: 0.0579 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9863\n",
      "Epoch 70: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0573 - accuracy: 0.9865 - val_loss: 0.0567 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9863\n",
      "Epoch 71: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0586 - accuracy: 0.9865 - val_loss: 0.0587 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0578 - accuracy: 0.9854\n",
      "Epoch 72: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0573 - accuracy: 0.9855 - val_loss: 0.0588 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0535 - accuracy: 0.9858\n",
      "Epoch 73: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0534 - accuracy: 0.9860 - val_loss: 0.0553 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9849\n",
      "Epoch 74: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0545 - accuracy: 0.9850 - val_loss: 0.0564 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9863\n",
      "Epoch 75: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0565 - accuracy: 0.9865 - val_loss: 0.0562 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9868\n",
      "Epoch 76: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0570 - accuracy: 0.9865 - val_loss: 0.0565 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0566 - accuracy: 0.9863\n",
      "Epoch 77: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0560 - accuracy: 0.9865 - val_loss: 0.0559 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0584 - accuracy: 0.9854\n",
      "Epoch 78: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0583 - accuracy: 0.9855 - val_loss: 0.0577 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9858\n",
      "Epoch 79: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0540 - accuracy: 0.9860 - val_loss: 0.0597 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9858\n",
      "Epoch 80: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0576 - accuracy: 0.9860 - val_loss: 0.0560 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9863\n",
      "Epoch 81: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0575 - accuracy: 0.9865 - val_loss: 0.0587 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0558 - accuracy: 0.9863\n",
      "Epoch 82: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0554 - accuracy: 0.9865 - val_loss: 0.0591 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9849\n",
      "Epoch 83: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0608 - accuracy: 0.9850 - val_loss: 0.0579 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0536 - accuracy: 0.9863\n",
      "Epoch 84: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0532 - accuracy: 0.9865 - val_loss: 0.0587 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0574 - accuracy: 0.9863\n",
      "Epoch 85: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0569 - accuracy: 0.9865 - val_loss: 0.0582 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0587 - accuracy: 0.9854\n",
      "Epoch 86: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0610 - accuracy: 0.9846 - val_loss: 0.0574 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9863\n",
      "Epoch 87: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0567 - accuracy: 0.9860 - val_loss: 0.0557 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9868\n",
      "Epoch 88: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.9865 - val_loss: 0.0542 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9854\n",
      "Epoch 89: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0533 - accuracy: 0.9850 - val_loss: 0.0575 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0564 - accuracy: 0.9858\n",
      "Epoch 90: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0558 - accuracy: 0.9860 - val_loss: 0.0555 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9863\n",
      "Epoch 91: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0570 - accuracy: 0.9865 - val_loss: 0.0571 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0506 - accuracy: 0.9863\n",
      "Epoch 92: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0506 - accuracy: 0.9865 - val_loss: 0.0561 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0527 - accuracy: 0.9863\n",
      "Epoch 93: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0522 - accuracy: 0.9865 - val_loss: 0.0553 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9863\n",
      "Epoch 94: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0565 - accuracy: 0.9865 - val_loss: 0.0572 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0477 - accuracy: 0.9858\n",
      "Epoch 95: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.9855 - val_loss: 0.0560 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9858\n",
      "Epoch 96: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0561 - accuracy: 0.9860 - val_loss: 0.0541 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0528 - accuracy: 0.9868\n",
      "Epoch 97: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0532 - accuracy: 0.9865 - val_loss: 0.0545 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9868\n",
      "Epoch 98: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0526 - accuracy: 0.9870 - val_loss: 0.0562 - val_accuracy: 0.9865 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0542 - accuracy: 0.9863\n",
      "Epoch 99: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0537 - accuracy: 0.9865 - val_loss: 0.0585 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "64/65 [============================>.] - ETA: 0s - loss: 0.0549 - accuracy: 0.9868\n",
      "Epoch 100: accuracy did not improve from 0.98745\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.0543 - accuracy: 0.9870 - val_loss: 0.0576 - val_accuracy: 0.9846 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f0f06763f40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 05:01:32.056013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 29ms/step - loss: 0.0815 - accuracy: 0.9838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08149749785661697, 0.9837837815284729]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_model = load_model('30_CNN1d_checkpoint.h5')\n",
    "cp_model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = CLASSES[np.argmax(y_pred, axis = -1)]\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(X_test)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    data = X_test[idx]\n",
    "    ax = fig.add_subplot(1, n_to_show, i+1)\n",
    "    ax.plot(data)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.2, 'pred = ' + str(preds_single[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n",
    "    ax.text(0.5, -0.4, 'act = ' + str(actual_single[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9836    0.9940    0.9887       662\n",
      "           1     0.9870    0.9838    0.9854       308\n",
      "           2     0.9776    0.9357    0.9562       140\n",
      "\n",
      "    accuracy                         0.9838      1110\n",
      "   macro avg     0.9827    0.9711    0.9768      1110\n",
      "weighted avg     0.9838    0.9838    0.9837      1110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_test_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        Benign  Sysrv  Xmrig\n",
      "Benign     658      2      2\n",
      "Sysrv        4    303      1\n",
      "Xmrig        7      2    131\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "class_labels = ['Benign', 'Sysrv', 'Xmrig']\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
