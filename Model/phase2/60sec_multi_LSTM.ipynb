{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:40:09.142822: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 22:40:09.193608: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 22:40:09.193646: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 22:40:09.193687: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 22:40:09.202729: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, LSTM, BatchNormalization, Dropout, Dense, Add, Flatten\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Calls List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "syscalls = [\n",
    "\"sys_enter_llistxattr\",\n",
    "\"sys_enter_setgroups\",\n",
    "\"sys_enter_lremovexattr\",\n",
    "\"sys_enter_sethostname\",\n",
    "\"sys_enter_accept\",\n",
    "\"sys_enter_lseek\",\n",
    "\"sys_enter_setitimer\",\n",
    "\"sys_enter_accept4\",\n",
    "\"sys_enter_lsetxattr\",\n",
    "\"sys_enter_setns\",\n",
    "\"sys_enter_acct\",\n",
    "\"sys_enter_madvise\",\n",
    "\"sys_enter_setpgid\",\n",
    "\"sys_enter_add_key\",\n",
    "\"sys_enter_mbind\",\n",
    "\"sys_enter_setpriority\",\n",
    "\"sys_enter_adjtimex\",\n",
    "\"sys_enter_membarrier\",\n",
    "\"sys_enter_setregid\",\n",
    "\"sys_enter_personality\",\n",
    "\"sys_enter_memfd_create\",\n",
    "\"sys_enter_setresgid\",\n",
    "\"sys_enter_bind\",\n",
    "\"sys_enter_memfd_secret\",\n",
    "\"sys_enter_setresuid\",\n",
    "\"sys_enter_bpf\",\n",
    "\"sys_enter_migrate_pages\",\n",
    "\"sys_enter_setreuid\",\n",
    "\"sys_enter_brk\",\n",
    "\"sys_enter_mincore\",\n",
    "\"sys_enter_setrlimit\",\n",
    "\"sys_enter_capget\",\n",
    "\"sys_enter_mkdirat\",\n",
    "\"sys_enter_setsid\",\n",
    "\"sys_enter_capset\",\n",
    "\"sys_enter_mknodat\",\n",
    "\"sys_enter_setsockopt\",\n",
    "\"sys_enter_chdir\",\n",
    "\"sys_enter_mlock\",\n",
    "\"sys_enter_settimeofday\",\n",
    "\"sys_enter_chroot\",\n",
    "\"sys_enter_mlock2\",\n",
    "\"sys_enter_setuid\",\n",
    "\"sys_enter_clock_adjtime\",\n",
    "\"sys_enter_mlockall\",\n",
    "\"sys_enter_setxattr\",\n",
    "\"sys_enter_clock_getres\",\n",
    "\"sys_enter_mmap\",\n",
    "\"sys_enter_shmat\",\n",
    "\"sys_enter_clock_gettime\",\n",
    "\"sys_enter_mount\",\n",
    "\"sys_enter_shmctl\",\n",
    "\"sys_enter_clock_nanosleep\",\n",
    "\"sys_enter_mount_setattr\",\n",
    "\"sys_enter_shmdt\",\n",
    "\"sys_enter_clock_settime\",\n",
    "\"sys_enter_move_mount\",\n",
    "\"sys_enter_shmget\",\n",
    "\"sys_enter_clone\",\n",
    "\"sys_enter_move_pages\",\n",
    "\"sys_enter_shutdown\",\n",
    "\"sys_enter_clone3\",\n",
    "\"sys_enter_mprotect\",\n",
    "\"sys_enter_sigaltstack\",\n",
    "\"sys_enter_close\",\n",
    "\"sys_enter_mq_getsetattr\",\n",
    "\"sys_enter_signalfd4\",\n",
    "\"sys_enter_close_range\",\n",
    "\"sys_enter_mq_notify\",\n",
    "\"sys_enter_socket\",\n",
    "\"sys_enter_connect\",\n",
    "\"sys_enter_mq_open\",\n",
    "\"sys_enter_socketpair\",\n",
    "\"sys_enter_copy_file_range\",\n",
    "\"sys_enter_mq_timedreceive\",\n",
    "\"sys_enter_splice\",\n",
    "\"sys_enter_delete_module\",\n",
    "\"sys_enter_mq_timedsend\",\n",
    "\"sys_enter_statfs\",\n",
    "\"sys_enter_dup\",\n",
    "\"sys_enter_mq_unlink\",\n",
    "\"sys_enter_statx\",\n",
    "\"sys_enter_dup3\",\n",
    "\"sys_enter_mremap\",\n",
    "\"sys_enter_swapoff\",\n",
    "\"sys_enter_epoll_create1\",\n",
    "\"sys_enter_msgctl\",\n",
    "\"sys_enter_swapon\",\n",
    "\"sys_enter_epoll_ctl\",\n",
    "\"sys_enter_msgget\",\n",
    "\"sys_enter_symlinkat\",\n",
    "\"sys_enter_epoll_pwait\",\n",
    "\"sys_enter_msgrcv\",\n",
    "\"sys_enter_sync\",\n",
    "\"sys_enter_epoll_pwait2\",\n",
    "\"sys_enter_msgsnd\",\n",
    "\"sys_enter_sync_file_range\",\n",
    "\"sys_enter_eventfd2\",\n",
    "\"sys_enter_msync\",\n",
    "\"sys_enter_syncfs\",\n",
    "\"sys_enter_execve\",\n",
    "\"sys_enter_munlock\",\n",
    "\"sys_enter_sysinfo\",\n",
    "\"sys_enter_execveat\",\n",
    "\"sys_enter_munlockall\",\n",
    "\"sys_enter_syslog\",\n",
    "\"sys_enter_exit\",\n",
    "\"sys_enter_munmap\",\n",
    "\"sys_enter_tee\",\n",
    "\"sys_enter_exit_group\",\n",
    "\"sys_enter_name_to_handle_at\",\n",
    "\"sys_enter_tgkill\",\n",
    "\"sys_enter_faccessat\",\n",
    "\"sys_enter_nanosleep\",\n",
    "\"sys_enter_timer_create\",\n",
    "\"sys_enter_faccessat2\",\n",
    "\"sys_enter_newfstat\",\n",
    "\"sys_enter_timer_delete\",\n",
    "\"sys_enter_fadvise64\",\n",
    "\"sys_enter_newfstatat\",\n",
    "\"sys_enter_timer_getoverrun\",\n",
    "\"sys_enter_fallocate\",\n",
    "\"sys_enter_newuname\",\n",
    "\"sys_enter_timer_gettime\",\n",
    "\"sys_enter_fanotify_init\",\n",
    "\"sys_enter_open_by_handle_at\",\n",
    "\"sys_enter_timer_settime\",\n",
    "\"sys_enter_fanotify_mark\",\n",
    "\"sys_enter_open_tree\",\n",
    "\"sys_enter_timerfd_create\",\n",
    "\"sys_enter_fchdir\",\n",
    "\"sys_enter_openat\",\n",
    "\"sys_enter_timerfd_gettime\",\n",
    "\"sys_enter_fchmod\",\n",
    "\"sys_enter_openat2\",\n",
    "\"sys_enter_timerfd_settime\",\n",
    "\"sys_enter_fchmodat\",\n",
    "\"sys_enter_perf_event_open\",\n",
    "\"sys_enter_times\",\n",
    "\"sys_enter_fchown\",\n",
    "\"sys_enter_pidfd_getfd\",\n",
    "\"sys_enter_tkill\",\n",
    "\"sys_enter_fchownat\",\n",
    "\"sys_enter_pidfd_open\",\n",
    "\"sys_enter_truncate\",\n",
    "\"sys_enter_fcntl\",\n",
    "\"sys_enter_pidfd_send_signal\",\n",
    "\"sys_enter_umask\",\n",
    "\"sys_enter_fdatasync\",\n",
    "\"sys_enter_pipe2\",\n",
    "\"sys_enter_umount\",\n",
    "\"sys_enter_fgetxattr\",\n",
    "\"sys_enter_pivot_root\",\n",
    "\"sys_enter_unlinkat\",\n",
    "\"sys_enter_finit_module\",\n",
    "\"sys_enter_ppoll\",\n",
    "\"sys_enter_unshare\",\n",
    "\"sys_enter_flistxattr\",\n",
    "\"sys_enter_prctl\",\n",
    "\"sys_enter_userfaultfd\",\n",
    "\"sys_enter_flock\",\n",
    "\"sys_enter_pread64\",\n",
    "\"sys_enter_utimensat\",\n",
    "\"sys_enter_fremovexattr\",\n",
    "\"sys_enter_preadv\",\n",
    "\"sys_enter_vhangup\",\n",
    "\"sys_enter_fsconfig\",\n",
    "\"sys_enter_preadv2\",\n",
    "\"sys_enter_vmsplice\",\n",
    "\"sys_enter_fsetxattr\",\n",
    "\"sys_enter_prlimit64\",\n",
    "\"sys_enter_wait4\",\n",
    "\"sys_enter_fsmount\",\n",
    "\"sys_enter_process_madvise\",\n",
    "\"sys_enter_waitid\",\n",
    "\"sys_enter_fsopen\",\n",
    "\"sys_enter_process_mrelease\",\n",
    "\"sys_enter_write\",\n",
    "\"sys_enter_fspick\",\n",
    "\"sys_enter_process_vm_readv\",\n",
    "\"sys_enter_writev\",\n",
    "\"sys_enter_fstatfs\",\n",
    "\"sys_enter_process_vm_writev\",\n",
    "\"sys_enter_fsync\",\n",
    "\"sys_enter_pselect6\",\n",
    "\"sys_enter_ftruncate\",\n",
    "\"sys_enter_ptrace\",\n",
    "\"sys_enter_futex\",\n",
    "\"sys_enter_pwrite64\",\n",
    "\"sys_enter_get_mempolicy\",\n",
    "\"sys_enter_pwritev\",\n",
    "\"sys_enter_get_robust_list\",\n",
    "\"sys_enter_pwritev2\",\n",
    "\"sys_enter_getcpu\",\n",
    "\"sys_enter_quotactl\",\n",
    "\"sys_enter_getcwd\",\n",
    "\"sys_enter_quotactl_fd\",\n",
    "\"sys_enter_getdents64\",\n",
    "\"sys_enter_read\",\n",
    "\"sys_enter_getegid\",\n",
    "\"sys_enter_readahead\",\n",
    "\"sys_enter_geteuid\",\n",
    "\"sys_enter_readlinkat\",\n",
    "\"sys_enter_getgid\",\n",
    "\"sys_enter_readv\",\n",
    "\"sys_enter_getgroups\",\n",
    "\"sys_enter_reboot\",\n",
    "\"sys_enter_getitimer\",\n",
    "\"sys_enter_recvfrom\",\n",
    "\"sys_enter_getpeername\",\n",
    "\"sys_enter_recvmmsg\",\n",
    "\"sys_enter_getpgid\",\n",
    "\"sys_enter_recvmsg\",\n",
    "\"sys_enter_getpid\",\n",
    "\"sys_enter_remap_file_pages\",\n",
    "\"sys_enter_getppid\",\n",
    "\"sys_enter_removexattr\",\n",
    "\"sys_enter_getpriority\",\n",
    "\"sys_enter_renameat\",\n",
    "\"sys_enter_getrandom\",\n",
    "\"sys_enter_renameat2\",\n",
    "\"sys_enter_getresgid\",\n",
    "\"sys_enter_request_key\",\n",
    "\"sys_enter_getresuid\",\n",
    "\"sys_enter_restart_syscall\",\n",
    "\"sys_enter_getrlimit\",\n",
    "\"sys_enter_rseq\",\n",
    "\"sys_enter_getrusage\",\n",
    "\"sys_enter_rt_sigaction\",\n",
    "\"sys_enter_getsid\",\n",
    "\"sys_enter_rt_sigpending\",\n",
    "\"sys_enter_getsockname\",\n",
    "\"sys_enter_rt_sigprocmask\",\n",
    "\"sys_enter_getsockopt\",\n",
    "\"sys_enter_rt_sigqueueinfo\",\n",
    "\"sys_enter_gettid\",\n",
    "\"sys_enter_rt_sigreturn\",\n",
    "\"sys_enter_gettimeofday\",\n",
    "\"sys_enter_rt_sigsuspend\",\n",
    "\"sys_enter_getuid\",\n",
    "\"sys_enter_rt_sigtimedwait\",\n",
    "\"sys_enter_getxattr\",\n",
    "\"sys_enter_rt_tgsigqueueinfo\",\n",
    "\"sys_enter_init_module\",\n",
    "\"sys_enter_sched_get_priority_max\",\n",
    "\"sys_enter_inotify_add_watch\",\n",
    "\"sys_enter_sched_get_priority_min\",\n",
    "\"sys_enter_inotify_init1\",\n",
    "\"sys_enter_sched_getaffinity\",\n",
    "\"sys_enter_inotify_rm_watch\",\n",
    "\"sys_enter_sched_getattr\",\n",
    "\"sys_enter_io_cancel\",\n",
    "\"sys_enter_sched_getparam\",\n",
    "\"sys_enter_io_destroy\",\n",
    "\"sys_enter_sched_getscheduler\",\n",
    "\"sys_enter_io_getevents\",\n",
    "\"sys_enter_sched_rr_get_interval\",\n",
    "\"sys_enter_io_pgetevents\",\n",
    "\"sys_enter_sched_setaffinity\",\n",
    "\"sys_enter_io_setup\",\n",
    "\"sys_enter_sched_setattr\",\n",
    "\"sys_enter_io_submit\",\n",
    "\"sys_enter_sched_setparam\",\n",
    "\"sys_enter_io_uring_enter\",\n",
    "\"sys_enter_sched_setscheduler\",\n",
    "\"sys_enter_io_uring_register\",\n",
    "\"sys_enter_sched_yield\",\n",
    "\"sys_enter_io_uring_setup\",\n",
    "\"sys_enter_seccomp\",\n",
    "\"sys_enter_ioctl\",\n",
    "\"sys_enter_semctl\",\n",
    "\"sys_enter_ioprio_get\",\n",
    "\"sys_enter_semget\",\n",
    "\"sys_enter_ioprio_set\",\n",
    "\"sys_enter_semop\",\n",
    "\"sys_enter_kcmp\",\n",
    "\"sys_enter_semtimedop\",\n",
    "\"sys_enter_kexec_file_load\",\n",
    "\"sys_enter_sendfile64\",\n",
    "\"sys_enter_kexec_load\",\n",
    "\"sys_enter_sendmmsg\",\n",
    "\"sys_enter_keyctl\",\n",
    "\"sys_enter_sendmsg\",\n",
    "\"sys_enter_kill\",\n",
    "\"sys_enter_sendto\",\n",
    "\"sys_enter_landlock_add_rule\",\n",
    "\"sys_enter_set_mempolicy\",\n",
    "\"sys_enter_landlock_create_ruleset\",\n",
    "\"sys_enter_set_robust_list\",\n",
    "\"sys_enter_landlock_restrict_self\",\n",
    "\"sys_enter_set_tid_address\",\n",
    "\"sys_enter_lgetxattr\",\n",
    "\"sys_enter_setdomainname\",\n",
    "\"sys_enter_linkat\",\n",
    "\"sys_enter_setfsgid\",\n",
    "\"sys_enter_listen\",\n",
    "\"sys_enter_setfsuid\",\n",
    "\"sys_enter_listxattr\",\n",
    "\"sys_enter_setgid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV from Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "CLASSES = np.array(['benign', 'sysrv', 'xmrig'])\n",
    "DATASET_DIR = \"raw_data/\"\n",
    "VECTOR_LENGTH = 32 * 32\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(syscalls)\n",
    "\n",
    "def csvToVector(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    data_encoded = label_encoder.fit_transform(data['SYSTEM_CALL'])\n",
    "    vector = np.zeros(VECTOR_LENGTH, dtype=np.uint8)\n",
    "    syscall_nums = min(len(data_encoded), VECTOR_LENGTH)\n",
    "    vector[:syscall_nums] = data_encoded[:syscall_nums]\n",
    "\n",
    "    return vector\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, class_idx = args\n",
    "    vector = csvToVector(file_path)\n",
    "    return vector, class_idx\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    classes = [\"0/60sec_0\", \"1/60sec_1\", \"2/60sec_2\"]\n",
    "\n",
    "    file_paths = []\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                file_paths.append((file_path, class_idx))\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_file, file_paths)\n",
    "\n",
    "    x, y = zip(*results)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Split and Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 299.0\n",
    "X_val = X_val / 299.0\n",
    "X_test = X_test / 299.0\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174, 1024)\n",
      "(630, 1024)\n",
      "(294, 1024)\n",
      "(1174, 3)\n",
      "(630, 3)\n",
      "(294, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 18:09:17.353370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-29 18:09:17.353910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-29 18:09:17.354395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(VECTOR_LENGTH, 1))\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(128, return_sequences=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1024, 32)          4352      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024, 32)          128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024, 32)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1024, 64)          24832     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 1024, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024, 64)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163715 (639.51 KB)\n",
      "Trainable params: 162755 (635.76 KB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/tmp/60_LSTM_checkpoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 18:09:23.942752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-07-29 18:09:25.313885: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f01fb464b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 18:09:25.313921: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 18:09:25.313928: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 18:09:25.313933: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 18:09:25.319713: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 18:09:25.410934: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 2.5624 - accuracy: 0.7385\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63605, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 13s 138ms/step - loss: 2.5624 - accuracy: 0.7385 - val_loss: 2.5596 - val_accuracy: 0.6361 - lr: 0.0010\n",
      "Epoch 2/100\n",
      " 1/37 [..............................] - ETA: 3s - loss: 2.3732 - accuracy: 0.7188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 2.2131 - accuracy: 0.7879\n",
      "Epoch 2: val_accuracy did not improve from 0.63605\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 2.2131 - accuracy: 0.7879 - val_loss: 2.4255 - val_accuracy: 0.6361 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 2.0585 - accuracy: 0.8041\n",
      "Epoch 3: val_accuracy improved from 0.63605 to 0.64626, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 2.0585 - accuracy: 0.8041 - val_loss: 2.3199 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.9468 - accuracy: 0.8032\n",
      "Epoch 4: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.9468 - accuracy: 0.8032 - val_loss: 2.2449 - val_accuracy: 0.1633 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.8139 - accuracy: 0.7973\n",
      "Epoch 5: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.8139 - accuracy: 0.7973 - val_loss: 2.2095 - val_accuracy: 0.1633 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6939 - accuracy: 0.8203\n",
      "Epoch 6: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.6939 - accuracy: 0.8203 - val_loss: 2.0236 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5714 - accuracy: 0.8305\n",
      "Epoch 7: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.5714 - accuracy: 0.8305 - val_loss: 2.0203 - val_accuracy: 0.1633 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5279 - accuracy: 0.8049\n",
      "Epoch 8: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.5279 - accuracy: 0.8049 - val_loss: 2.4497 - val_accuracy: 0.1531 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.3994 - accuracy: 0.8279\n",
      "Epoch 9: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.3994 - accuracy: 0.8279 - val_loss: 2.1567 - val_accuracy: 0.1531 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.2640 - accuracy: 0.8330\n",
      "Epoch 10: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.2640 - accuracy: 0.8330 - val_loss: 2.9769 - val_accuracy: 0.1531 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.2375 - accuracy: 0.8279\n",
      "Epoch 11: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.2375 - accuracy: 0.8279 - val_loss: 1.6713 - val_accuracy: 0.6293 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.1529 - accuracy: 0.8433\n",
      "Epoch 12: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.1529 - accuracy: 0.8433 - val_loss: 2.4627 - val_accuracy: 0.3163 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.0707 - accuracy: 0.8535\n",
      "Epoch 13: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.0707 - accuracy: 0.8535 - val_loss: 2.6113 - val_accuracy: 0.1667 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.0545 - accuracy: 0.8296\n",
      "Epoch 14: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.0545 - accuracy: 0.8296 - val_loss: 1.9528 - val_accuracy: 0.1735 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.0158 - accuracy: 0.8433\n",
      "Epoch 15: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.0158 - accuracy: 0.8433 - val_loss: 2.2095 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9623 - accuracy: 0.8433\n",
      "Epoch 16: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.9623 - accuracy: 0.8433 - val_loss: 2.1247 - val_accuracy: 0.3333 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9285 - accuracy: 0.8399\n",
      "Epoch 17: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.9285 - accuracy: 0.8399 - val_loss: 1.4061 - val_accuracy: 0.6190 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8588 - accuracy: 0.8535\n",
      "Epoch 18: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.8588 - accuracy: 0.8535 - val_loss: 2.7538 - val_accuracy: 0.2109 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8746 - accuracy: 0.8467\n",
      "Epoch 19: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.8746 - accuracy: 0.8467 - val_loss: 1.4041 - val_accuracy: 0.6224 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8266 - accuracy: 0.8552\n",
      "Epoch 20: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.8266 - accuracy: 0.8552 - val_loss: 2.0705 - val_accuracy: 0.2279 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.8671\n",
      "Epoch 21: val_accuracy did not improve from 0.64626\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.7912 - accuracy: 0.8671 - val_loss: 1.4896 - val_accuracy: 0.2619 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8135 - accuracy: 0.8501\n",
      "Epoch 22: val_accuracy improved from 0.64626 to 0.65306, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 0.8135 - accuracy: 0.8501 - val_loss: 1.6348 - val_accuracy: 0.6531 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7961 - accuracy: 0.8407\n",
      "Epoch 23: val_accuracy did not improve from 0.65306\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.7961 - accuracy: 0.8407 - val_loss: 2.0355 - val_accuracy: 0.1973 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7829 - accuracy: 0.8416\n",
      "Epoch 24: val_accuracy did not improve from 0.65306\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.7829 - accuracy: 0.8416 - val_loss: 1.8171 - val_accuracy: 0.1973 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7576 - accuracy: 0.8467\n",
      "Epoch 25: val_accuracy did not improve from 0.65306\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.7576 - accuracy: 0.8467 - val_loss: 2.0108 - val_accuracy: 0.2109 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7136 - accuracy: 0.8629\n",
      "Epoch 26: val_accuracy did not improve from 0.65306\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.7136 - accuracy: 0.8629 - val_loss: 1.9084 - val_accuracy: 0.2109 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7100 - accuracy: 0.8518\n",
      "Epoch 27: val_accuracy did not improve from 0.65306\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.7100 - accuracy: 0.8518 - val_loss: 1.9413 - val_accuracy: 0.2109 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7266 - accuracy: 0.8705\n",
      "Epoch 28: val_accuracy improved from 0.65306 to 0.70068, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 0.7266 - accuracy: 0.8705 - val_loss: 1.2798 - val_accuracy: 0.7007 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6977 - accuracy: 0.8569\n",
      "Epoch 29: val_accuracy improved from 0.70068 to 0.88095, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6977 - accuracy: 0.8569 - val_loss: 0.6434 - val_accuracy: 0.8810 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.8731\n",
      "Epoch 30: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6679 - accuracy: 0.8731 - val_loss: 1.6904 - val_accuracy: 0.2109 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.8595\n",
      "Epoch 31: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6963 - accuracy: 0.8595 - val_loss: 1.2646 - val_accuracy: 0.7041 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.8629\n",
      "Epoch 32: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6696 - accuracy: 0.8629 - val_loss: 1.0905 - val_accuracy: 0.7381 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6624 - accuracy: 0.8629\n",
      "Epoch 33: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6624 - accuracy: 0.8629 - val_loss: 0.9081 - val_accuracy: 0.7925 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6371 - accuracy: 0.8705\n",
      "Epoch 34: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6371 - accuracy: 0.8705 - val_loss: 1.3387 - val_accuracy: 0.6871 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.8739\n",
      "Epoch 35: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6357 - accuracy: 0.8739 - val_loss: 0.8002 - val_accuracy: 0.8061 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6421 - accuracy: 0.8663\n",
      "Epoch 36: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6421 - accuracy: 0.8663 - val_loss: 0.8822 - val_accuracy: 0.7959 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6366 - accuracy: 0.8773\n",
      "Epoch 37: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6366 - accuracy: 0.8773 - val_loss: 0.9527 - val_accuracy: 0.7653 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6265 - accuracy: 0.8731\n",
      "Epoch 38: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6265 - accuracy: 0.8731 - val_loss: 0.7142 - val_accuracy: 0.8095 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6189 - accuracy: 0.8612\n",
      "Epoch 39: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6189 - accuracy: 0.8612 - val_loss: 0.6092 - val_accuracy: 0.8673 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6014 - accuracy: 0.8697\n",
      "Epoch 40: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6014 - accuracy: 0.8697 - val_loss: 1.6994 - val_accuracy: 0.2551 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6165 - accuracy: 0.8688\n",
      "Epoch 41: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6165 - accuracy: 0.8688 - val_loss: 0.6161 - val_accuracy: 0.8605 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6304 - accuracy: 0.8680\n",
      "Epoch 42: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6304 - accuracy: 0.8680 - val_loss: 0.6188 - val_accuracy: 0.8605 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.8680\n",
      "Epoch 43: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6202 - accuracy: 0.8680 - val_loss: 0.6090 - val_accuracy: 0.8673 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.8475\n",
      "Epoch 44: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6559 - accuracy: 0.8475 - val_loss: 1.0659 - val_accuracy: 0.7177 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6447 - accuracy: 0.8424\n",
      "Epoch 45: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6447 - accuracy: 0.8424 - val_loss: 1.3822 - val_accuracy: 0.7177 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.8697\n",
      "Epoch 46: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6039 - accuracy: 0.8697 - val_loss: 1.5759 - val_accuracy: 0.6837 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6032 - accuracy: 0.8714\n",
      "Epoch 47: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6032 - accuracy: 0.8714 - val_loss: 1.4741 - val_accuracy: 0.6905 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6024 - accuracy: 0.8680\n",
      "Epoch 48: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6024 - accuracy: 0.8680 - val_loss: 1.1877 - val_accuracy: 0.7381 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.8748\n",
      "Epoch 49: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5584 - accuracy: 0.8748 - val_loss: 0.7515 - val_accuracy: 0.8061 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5909 - accuracy: 0.8697\n",
      "Epoch 50: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5909 - accuracy: 0.8697 - val_loss: 0.5503 - val_accuracy: 0.8673 - lr: 6.2500e-05\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5840 - accuracy: 0.8714\n",
      "Epoch 51: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5840 - accuracy: 0.8714 - val_loss: 0.7230 - val_accuracy: 0.8163 - lr: 6.2500e-05\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.8654\n",
      "Epoch 52: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5975 - accuracy: 0.8654 - val_loss: 1.3178 - val_accuracy: 0.2959 - lr: 6.2500e-05\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5572 - accuracy: 0.8756\n",
      "Epoch 53: val_accuracy did not improve from 0.88095\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5572 - accuracy: 0.8756 - val_loss: 0.7149 - val_accuracy: 0.8163 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.8714\n",
      "Epoch 54: val_accuracy improved from 0.88095 to 0.89116, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 0.5648 - accuracy: 0.8714 - val_loss: 0.5441 - val_accuracy: 0.8912 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5767 - accuracy: 0.8620\n",
      "Epoch 55: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5767 - accuracy: 0.8620 - val_loss: 0.5471 - val_accuracy: 0.8878 - lr: 6.2500e-05\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5714 - accuracy: 0.8722\n",
      "Epoch 56: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5714 - accuracy: 0.8722 - val_loss: 0.5479 - val_accuracy: 0.8912 - lr: 6.2500e-05\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5416 - accuracy: 0.8799\n",
      "Epoch 57: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5416 - accuracy: 0.8799 - val_loss: 0.6174 - val_accuracy: 0.8367 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8629\n",
      "Epoch 58: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5644 - accuracy: 0.8629 - val_loss: 0.5238 - val_accuracy: 0.8673 - lr: 6.2500e-05\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.8680\n",
      "Epoch 59: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5551 - accuracy: 0.8680 - val_loss: 0.7471 - val_accuracy: 0.8129 - lr: 6.2500e-05\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.8637\n",
      "Epoch 60: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5664 - accuracy: 0.8637 - val_loss: 0.9102 - val_accuracy: 0.8095 - lr: 6.2500e-05\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.8663\n",
      "Epoch 61: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5715 - accuracy: 0.8663 - val_loss: 1.1374 - val_accuracy: 0.3231 - lr: 6.2500e-05\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.8739\n",
      "Epoch 62: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5483 - accuracy: 0.8739 - val_loss: 0.6403 - val_accuracy: 0.8163 - lr: 6.2500e-05\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.8799\n",
      "Epoch 63: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5532 - accuracy: 0.8799 - val_loss: 0.5161 - val_accuracy: 0.8707 - lr: 6.2500e-05\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.8697\n",
      "Epoch 64: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5597 - accuracy: 0.8697 - val_loss: 0.6002 - val_accuracy: 0.8605 - lr: 6.2500e-05\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5430 - accuracy: 0.8773\n",
      "Epoch 65: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5430 - accuracy: 0.8773 - val_loss: 1.0910 - val_accuracy: 0.7585 - lr: 6.2500e-05\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5559 - accuracy: 0.8748\n",
      "Epoch 66: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5559 - accuracy: 0.8748 - val_loss: 1.7945 - val_accuracy: 0.2109 - lr: 6.2500e-05\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.8782\n",
      "Epoch 67: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5446 - accuracy: 0.8782 - val_loss: 1.3909 - val_accuracy: 0.2959 - lr: 6.2500e-05\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.8680\n",
      "Epoch 68: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5480 - accuracy: 0.8680 - val_loss: 0.7619 - val_accuracy: 0.7755 - lr: 6.2500e-05\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5565 - accuracy: 0.8705\n",
      "Epoch 69: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5565 - accuracy: 0.8705 - val_loss: 0.5389 - val_accuracy: 0.8673 - lr: 3.1250e-05\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.8705\n",
      "Epoch 70: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5453 - accuracy: 0.8705 - val_loss: 0.5422 - val_accuracy: 0.8776 - lr: 3.1250e-05\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.8782\n",
      "Epoch 71: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5447 - accuracy: 0.8782 - val_loss: 0.5271 - val_accuracy: 0.8776 - lr: 3.1250e-05\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5208 - accuracy: 0.8714\n",
      "Epoch 72: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5208 - accuracy: 0.8714 - val_loss: 0.5001 - val_accuracy: 0.8707 - lr: 3.1250e-05\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4977 - accuracy: 0.8842\n",
      "Epoch 73: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4977 - accuracy: 0.8842 - val_loss: 0.5184 - val_accuracy: 0.8810 - lr: 3.1250e-05\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.8680\n",
      "Epoch 74: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5558 - accuracy: 0.8680 - val_loss: 0.5719 - val_accuracy: 0.8673 - lr: 3.1250e-05\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.8825\n",
      "Epoch 75: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5345 - accuracy: 0.8825 - val_loss: 0.6129 - val_accuracy: 0.8367 - lr: 3.1250e-05\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.8731\n",
      "Epoch 76: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5304 - accuracy: 0.8731 - val_loss: 0.6921 - val_accuracy: 0.8129 - lr: 3.1250e-05\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5336 - accuracy: 0.8773\n",
      "Epoch 77: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5336 - accuracy: 0.8773 - val_loss: 0.6192 - val_accuracy: 0.8333 - lr: 3.1250e-05\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.8765\n",
      "Epoch 78: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5298 - accuracy: 0.8765 - val_loss: 0.5436 - val_accuracy: 0.8741 - lr: 1.5625e-05\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5400 - accuracy: 0.8748\n",
      "Epoch 79: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5400 - accuracy: 0.8748 - val_loss: 0.5235 - val_accuracy: 0.8776 - lr: 1.5625e-05\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.8799\n",
      "Epoch 80: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5132 - accuracy: 0.8799 - val_loss: 0.5011 - val_accuracy: 0.8741 - lr: 1.5625e-05\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5265 - accuracy: 0.8807\n",
      "Epoch 81: val_accuracy improved from 0.89116 to 0.89796, saving model to /tmp/60_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 0.5265 - accuracy: 0.8807 - val_loss: 0.5019 - val_accuracy: 0.8980 - lr: 1.5625e-05\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.8722\n",
      "Epoch 82: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5284 - accuracy: 0.8722 - val_loss: 0.4982 - val_accuracy: 0.8707 - lr: 1.5625e-05\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.8697\n",
      "Epoch 83: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5485 - accuracy: 0.8697 - val_loss: 0.5022 - val_accuracy: 0.8980 - lr: 1.5625e-05\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5284 - accuracy: 0.8663\n",
      "Epoch 84: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5284 - accuracy: 0.8663 - val_loss: 0.5026 - val_accuracy: 0.8946 - lr: 1.5625e-05\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.8697\n",
      "Epoch 85: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5337 - accuracy: 0.8697 - val_loss: 0.5081 - val_accuracy: 0.8912 - lr: 1.5625e-05\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.8790\n",
      "Epoch 86: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5297 - accuracy: 0.8790 - val_loss: 0.4963 - val_accuracy: 0.8707 - lr: 1.5625e-05\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.8765\n",
      "Epoch 87: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5391 - accuracy: 0.8765 - val_loss: 0.4980 - val_accuracy: 0.8741 - lr: 1.5625e-05\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.8825\n",
      "Epoch 88: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5035 - accuracy: 0.8825 - val_loss: 0.5217 - val_accuracy: 0.8912 - lr: 1.5625e-05\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5151 - accuracy: 0.8756\n",
      "Epoch 89: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5151 - accuracy: 0.8756 - val_loss: 0.6047 - val_accuracy: 0.8163 - lr: 1.5625e-05\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5299 - accuracy: 0.8739\n",
      "Epoch 90: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5299 - accuracy: 0.8739 - val_loss: 0.5760 - val_accuracy: 0.8605 - lr: 1.5625e-05\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.8705\n",
      "Epoch 91: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5263 - accuracy: 0.8705 - val_loss: 0.5222 - val_accuracy: 0.8912 - lr: 1.5625e-05\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5196 - accuracy: 0.8790\n",
      "Epoch 92: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5196 - accuracy: 0.8790 - val_loss: 0.5239 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5181 - accuracy: 0.8756\n",
      "Epoch 93: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5181 - accuracy: 0.8756 - val_loss: 0.5685 - val_accuracy: 0.8639 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5245 - accuracy: 0.8807\n",
      "Epoch 94: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5245 - accuracy: 0.8807 - val_loss: 0.5002 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5213 - accuracy: 0.8773\n",
      "Epoch 95: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5213 - accuracy: 0.8773 - val_loss: 0.4992 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.8790\n",
      "Epoch 96: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5169 - accuracy: 0.8790 - val_loss: 0.4950 - val_accuracy: 0.8707 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5381 - accuracy: 0.8688\n",
      "Epoch 97: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5381 - accuracy: 0.8688 - val_loss: 0.4897 - val_accuracy: 0.8707 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.8773\n",
      "Epoch 98: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5198 - accuracy: 0.8773 - val_loss: 0.5013 - val_accuracy: 0.8844 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.8722\n",
      "Epoch 99: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5305 - accuracy: 0.8722 - val_loss: 0.4928 - val_accuracy: 0.8776 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5195 - accuracy: 0.8714\n",
      "Epoch 100: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5195 - accuracy: 0.8714 - val_loss: 0.4922 - val_accuracy: 0.8741 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f041021aaa0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:40:28.879288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-31 22:40:28.880452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-31 22:40:28.880952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2024-07-31 22:40:31.727210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.4982 - accuracy: 0.8937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4982200562953949, 0.8936507701873779]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_model = load_model('/tmp/60_LSTM_checkpoint.h5')\n",
    "cp_model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = CLASSES[np.argmax(y_pred, axis = -1)]\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(X_test)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    data = X_test[idx]\n",
    "    ax = fig.add_subplot(1, n_to_show, i+1)\n",
    "    ax.plot(data)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.2, 'pred = ' + str(preds_single[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n",
    "    ax.text(0.5, -0.4, 'act = ' + str(actual_single[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9576    0.8968    0.9262       378\n",
      "           1     0.9085    0.8968    0.9026       155\n",
      "           2     0.6911    0.8763    0.7727        97\n",
      "\n",
      "    accuracy                         0.8937       630\n",
      "   macro avg     0.8524    0.8900    0.8672       630\n",
      "weighted avg     0.9045    0.8937    0.8968       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_test_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        Benign  Sysrv  Xmrig\n",
      "Benign     339     13     26\n",
      "Sysrv        4    139     12\n",
      "Xmrig       11      1     85\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "class_labels = ['Benign', 'Sysrv', 'Xmrig']\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
