{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:43:01.599979: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 22:43:01.651119: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 22:43:01.651160: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 22:43:01.651193: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 22:43:01.660241: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, LSTM, BatchNormalization, Dropout, Dense, Add, Flatten\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Calls List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "syscalls = [\n",
    "\"sys_enter_llistxattr\",\n",
    "\"sys_enter_setgroups\",\n",
    "\"sys_enter_lremovexattr\",\n",
    "\"sys_enter_sethostname\",\n",
    "\"sys_enter_accept\",\n",
    "\"sys_enter_lseek\",\n",
    "\"sys_enter_setitimer\",\n",
    "\"sys_enter_accept4\",\n",
    "\"sys_enter_lsetxattr\",\n",
    "\"sys_enter_setns\",\n",
    "\"sys_enter_acct\",\n",
    "\"sys_enter_madvise\",\n",
    "\"sys_enter_setpgid\",\n",
    "\"sys_enter_add_key\",\n",
    "\"sys_enter_mbind\",\n",
    "\"sys_enter_setpriority\",\n",
    "\"sys_enter_adjtimex\",\n",
    "\"sys_enter_membarrier\",\n",
    "\"sys_enter_setregid\",\n",
    "\"sys_enter_personality\",\n",
    "\"sys_enter_memfd_create\",\n",
    "\"sys_enter_setresgid\",\n",
    "\"sys_enter_bind\",\n",
    "\"sys_enter_memfd_secret\",\n",
    "\"sys_enter_setresuid\",\n",
    "\"sys_enter_bpf\",\n",
    "\"sys_enter_migrate_pages\",\n",
    "\"sys_enter_setreuid\",\n",
    "\"sys_enter_brk\",\n",
    "\"sys_enter_mincore\",\n",
    "\"sys_enter_setrlimit\",\n",
    "\"sys_enter_capget\",\n",
    "\"sys_enter_mkdirat\",\n",
    "\"sys_enter_setsid\",\n",
    "\"sys_enter_capset\",\n",
    "\"sys_enter_mknodat\",\n",
    "\"sys_enter_setsockopt\",\n",
    "\"sys_enter_chdir\",\n",
    "\"sys_enter_mlock\",\n",
    "\"sys_enter_settimeofday\",\n",
    "\"sys_enter_chroot\",\n",
    "\"sys_enter_mlock2\",\n",
    "\"sys_enter_setuid\",\n",
    "\"sys_enter_clock_adjtime\",\n",
    "\"sys_enter_mlockall\",\n",
    "\"sys_enter_setxattr\",\n",
    "\"sys_enter_clock_getres\",\n",
    "\"sys_enter_mmap\",\n",
    "\"sys_enter_shmat\",\n",
    "\"sys_enter_clock_gettime\",\n",
    "\"sys_enter_mount\",\n",
    "\"sys_enter_shmctl\",\n",
    "\"sys_enter_clock_nanosleep\",\n",
    "\"sys_enter_mount_setattr\",\n",
    "\"sys_enter_shmdt\",\n",
    "\"sys_enter_clock_settime\",\n",
    "\"sys_enter_move_mount\",\n",
    "\"sys_enter_shmget\",\n",
    "\"sys_enter_clone\",\n",
    "\"sys_enter_move_pages\",\n",
    "\"sys_enter_shutdown\",\n",
    "\"sys_enter_clone3\",\n",
    "\"sys_enter_mprotect\",\n",
    "\"sys_enter_sigaltstack\",\n",
    "\"sys_enter_close\",\n",
    "\"sys_enter_mq_getsetattr\",\n",
    "\"sys_enter_signalfd4\",\n",
    "\"sys_enter_close_range\",\n",
    "\"sys_enter_mq_notify\",\n",
    "\"sys_enter_socket\",\n",
    "\"sys_enter_connect\",\n",
    "\"sys_enter_mq_open\",\n",
    "\"sys_enter_socketpair\",\n",
    "\"sys_enter_copy_file_range\",\n",
    "\"sys_enter_mq_timedreceive\",\n",
    "\"sys_enter_splice\",\n",
    "\"sys_enter_delete_module\",\n",
    "\"sys_enter_mq_timedsend\",\n",
    "\"sys_enter_statfs\",\n",
    "\"sys_enter_dup\",\n",
    "\"sys_enter_mq_unlink\",\n",
    "\"sys_enter_statx\",\n",
    "\"sys_enter_dup3\",\n",
    "\"sys_enter_mremap\",\n",
    "\"sys_enter_swapoff\",\n",
    "\"sys_enter_epoll_create1\",\n",
    "\"sys_enter_msgctl\",\n",
    "\"sys_enter_swapon\",\n",
    "\"sys_enter_epoll_ctl\",\n",
    "\"sys_enter_msgget\",\n",
    "\"sys_enter_symlinkat\",\n",
    "\"sys_enter_epoll_pwait\",\n",
    "\"sys_enter_msgrcv\",\n",
    "\"sys_enter_sync\",\n",
    "\"sys_enter_epoll_pwait2\",\n",
    "\"sys_enter_msgsnd\",\n",
    "\"sys_enter_sync_file_range\",\n",
    "\"sys_enter_eventfd2\",\n",
    "\"sys_enter_msync\",\n",
    "\"sys_enter_syncfs\",\n",
    "\"sys_enter_execve\",\n",
    "\"sys_enter_munlock\",\n",
    "\"sys_enter_sysinfo\",\n",
    "\"sys_enter_execveat\",\n",
    "\"sys_enter_munlockall\",\n",
    "\"sys_enter_syslog\",\n",
    "\"sys_enter_exit\",\n",
    "\"sys_enter_munmap\",\n",
    "\"sys_enter_tee\",\n",
    "\"sys_enter_exit_group\",\n",
    "\"sys_enter_name_to_handle_at\",\n",
    "\"sys_enter_tgkill\",\n",
    "\"sys_enter_faccessat\",\n",
    "\"sys_enter_nanosleep\",\n",
    "\"sys_enter_timer_create\",\n",
    "\"sys_enter_faccessat2\",\n",
    "\"sys_enter_newfstat\",\n",
    "\"sys_enter_timer_delete\",\n",
    "\"sys_enter_fadvise64\",\n",
    "\"sys_enter_newfstatat\",\n",
    "\"sys_enter_timer_getoverrun\",\n",
    "\"sys_enter_fallocate\",\n",
    "\"sys_enter_newuname\",\n",
    "\"sys_enter_timer_gettime\",\n",
    "\"sys_enter_fanotify_init\",\n",
    "\"sys_enter_open_by_handle_at\",\n",
    "\"sys_enter_timer_settime\",\n",
    "\"sys_enter_fanotify_mark\",\n",
    "\"sys_enter_open_tree\",\n",
    "\"sys_enter_timerfd_create\",\n",
    "\"sys_enter_fchdir\",\n",
    "\"sys_enter_openat\",\n",
    "\"sys_enter_timerfd_gettime\",\n",
    "\"sys_enter_fchmod\",\n",
    "\"sys_enter_openat2\",\n",
    "\"sys_enter_timerfd_settime\",\n",
    "\"sys_enter_fchmodat\",\n",
    "\"sys_enter_perf_event_open\",\n",
    "\"sys_enter_times\",\n",
    "\"sys_enter_fchown\",\n",
    "\"sys_enter_pidfd_getfd\",\n",
    "\"sys_enter_tkill\",\n",
    "\"sys_enter_fchownat\",\n",
    "\"sys_enter_pidfd_open\",\n",
    "\"sys_enter_truncate\",\n",
    "\"sys_enter_fcntl\",\n",
    "\"sys_enter_pidfd_send_signal\",\n",
    "\"sys_enter_umask\",\n",
    "\"sys_enter_fdatasync\",\n",
    "\"sys_enter_pipe2\",\n",
    "\"sys_enter_umount\",\n",
    "\"sys_enter_fgetxattr\",\n",
    "\"sys_enter_pivot_root\",\n",
    "\"sys_enter_unlinkat\",\n",
    "\"sys_enter_finit_module\",\n",
    "\"sys_enter_ppoll\",\n",
    "\"sys_enter_unshare\",\n",
    "\"sys_enter_flistxattr\",\n",
    "\"sys_enter_prctl\",\n",
    "\"sys_enter_userfaultfd\",\n",
    "\"sys_enter_flock\",\n",
    "\"sys_enter_pread64\",\n",
    "\"sys_enter_utimensat\",\n",
    "\"sys_enter_fremovexattr\",\n",
    "\"sys_enter_preadv\",\n",
    "\"sys_enter_vhangup\",\n",
    "\"sys_enter_fsconfig\",\n",
    "\"sys_enter_preadv2\",\n",
    "\"sys_enter_vmsplice\",\n",
    "\"sys_enter_fsetxattr\",\n",
    "\"sys_enter_prlimit64\",\n",
    "\"sys_enter_wait4\",\n",
    "\"sys_enter_fsmount\",\n",
    "\"sys_enter_process_madvise\",\n",
    "\"sys_enter_waitid\",\n",
    "\"sys_enter_fsopen\",\n",
    "\"sys_enter_process_mrelease\",\n",
    "\"sys_enter_write\",\n",
    "\"sys_enter_fspick\",\n",
    "\"sys_enter_process_vm_readv\",\n",
    "\"sys_enter_writev\",\n",
    "\"sys_enter_fstatfs\",\n",
    "\"sys_enter_process_vm_writev\",\n",
    "\"sys_enter_fsync\",\n",
    "\"sys_enter_pselect6\",\n",
    "\"sys_enter_ftruncate\",\n",
    "\"sys_enter_ptrace\",\n",
    "\"sys_enter_futex\",\n",
    "\"sys_enter_pwrite64\",\n",
    "\"sys_enter_get_mempolicy\",\n",
    "\"sys_enter_pwritev\",\n",
    "\"sys_enter_get_robust_list\",\n",
    "\"sys_enter_pwritev2\",\n",
    "\"sys_enter_getcpu\",\n",
    "\"sys_enter_quotactl\",\n",
    "\"sys_enter_getcwd\",\n",
    "\"sys_enter_quotactl_fd\",\n",
    "\"sys_enter_getdents64\",\n",
    "\"sys_enter_read\",\n",
    "\"sys_enter_getegid\",\n",
    "\"sys_enter_readahead\",\n",
    "\"sys_enter_geteuid\",\n",
    "\"sys_enter_readlinkat\",\n",
    "\"sys_enter_getgid\",\n",
    "\"sys_enter_readv\",\n",
    "\"sys_enter_getgroups\",\n",
    "\"sys_enter_reboot\",\n",
    "\"sys_enter_getitimer\",\n",
    "\"sys_enter_recvfrom\",\n",
    "\"sys_enter_getpeername\",\n",
    "\"sys_enter_recvmmsg\",\n",
    "\"sys_enter_getpgid\",\n",
    "\"sys_enter_recvmsg\",\n",
    "\"sys_enter_getpid\",\n",
    "\"sys_enter_remap_file_pages\",\n",
    "\"sys_enter_getppid\",\n",
    "\"sys_enter_removexattr\",\n",
    "\"sys_enter_getpriority\",\n",
    "\"sys_enter_renameat\",\n",
    "\"sys_enter_getrandom\",\n",
    "\"sys_enter_renameat2\",\n",
    "\"sys_enter_getresgid\",\n",
    "\"sys_enter_request_key\",\n",
    "\"sys_enter_getresuid\",\n",
    "\"sys_enter_restart_syscall\",\n",
    "\"sys_enter_getrlimit\",\n",
    "\"sys_enter_rseq\",\n",
    "\"sys_enter_getrusage\",\n",
    "\"sys_enter_rt_sigaction\",\n",
    "\"sys_enter_getsid\",\n",
    "\"sys_enter_rt_sigpending\",\n",
    "\"sys_enter_getsockname\",\n",
    "\"sys_enter_rt_sigprocmask\",\n",
    "\"sys_enter_getsockopt\",\n",
    "\"sys_enter_rt_sigqueueinfo\",\n",
    "\"sys_enter_gettid\",\n",
    "\"sys_enter_rt_sigreturn\",\n",
    "\"sys_enter_gettimeofday\",\n",
    "\"sys_enter_rt_sigsuspend\",\n",
    "\"sys_enter_getuid\",\n",
    "\"sys_enter_rt_sigtimedwait\",\n",
    "\"sys_enter_getxattr\",\n",
    "\"sys_enter_rt_tgsigqueueinfo\",\n",
    "\"sys_enter_init_module\",\n",
    "\"sys_enter_sched_get_priority_max\",\n",
    "\"sys_enter_inotify_add_watch\",\n",
    "\"sys_enter_sched_get_priority_min\",\n",
    "\"sys_enter_inotify_init1\",\n",
    "\"sys_enter_sched_getaffinity\",\n",
    "\"sys_enter_inotify_rm_watch\",\n",
    "\"sys_enter_sched_getattr\",\n",
    "\"sys_enter_io_cancel\",\n",
    "\"sys_enter_sched_getparam\",\n",
    "\"sys_enter_io_destroy\",\n",
    "\"sys_enter_sched_getscheduler\",\n",
    "\"sys_enter_io_getevents\",\n",
    "\"sys_enter_sched_rr_get_interval\",\n",
    "\"sys_enter_io_pgetevents\",\n",
    "\"sys_enter_sched_setaffinity\",\n",
    "\"sys_enter_io_setup\",\n",
    "\"sys_enter_sched_setattr\",\n",
    "\"sys_enter_io_submit\",\n",
    "\"sys_enter_sched_setparam\",\n",
    "\"sys_enter_io_uring_enter\",\n",
    "\"sys_enter_sched_setscheduler\",\n",
    "\"sys_enter_io_uring_register\",\n",
    "\"sys_enter_sched_yield\",\n",
    "\"sys_enter_io_uring_setup\",\n",
    "\"sys_enter_seccomp\",\n",
    "\"sys_enter_ioctl\",\n",
    "\"sys_enter_semctl\",\n",
    "\"sys_enter_ioprio_get\",\n",
    "\"sys_enter_semget\",\n",
    "\"sys_enter_ioprio_set\",\n",
    "\"sys_enter_semop\",\n",
    "\"sys_enter_kcmp\",\n",
    "\"sys_enter_semtimedop\",\n",
    "\"sys_enter_kexec_file_load\",\n",
    "\"sys_enter_sendfile64\",\n",
    "\"sys_enter_kexec_load\",\n",
    "\"sys_enter_sendmmsg\",\n",
    "\"sys_enter_keyctl\",\n",
    "\"sys_enter_sendmsg\",\n",
    "\"sys_enter_kill\",\n",
    "\"sys_enter_sendto\",\n",
    "\"sys_enter_landlock_add_rule\",\n",
    "\"sys_enter_set_mempolicy\",\n",
    "\"sys_enter_landlock_create_ruleset\",\n",
    "\"sys_enter_set_robust_list\",\n",
    "\"sys_enter_landlock_restrict_self\",\n",
    "\"sys_enter_set_tid_address\",\n",
    "\"sys_enter_lgetxattr\",\n",
    "\"sys_enter_setdomainname\",\n",
    "\"sys_enter_linkat\",\n",
    "\"sys_enter_setfsgid\",\n",
    "\"sys_enter_listen\",\n",
    "\"sys_enter_setfsuid\",\n",
    "\"sys_enter_listxattr\",\n",
    "\"sys_enter_setgid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV from Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "CLASSES = np.array(['benign', 'sysrv', 'xmrig'])\n",
    "DATASET_DIR = \"raw_data/\"\n",
    "VECTOR_LENGTH = 32 * 32\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(syscalls)\n",
    "\n",
    "def csvToVector(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    data_encoded = label_encoder.fit_transform(data['SYSTEM_CALL'])\n",
    "    vector = np.zeros(VECTOR_LENGTH, dtype=np.uint8)\n",
    "    syscall_nums = min(len(data_encoded), VECTOR_LENGTH)\n",
    "    vector[:syscall_nums] = data_encoded[:syscall_nums]\n",
    "\n",
    "    return vector\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, class_idx = args\n",
    "    vector = csvToVector(file_path)\n",
    "    return vector, class_idx\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    classes = [\"0/60sec_0\", \"1/60sec_1\", \"2/60sec_2\"]\n",
    "\n",
    "    file_paths = []\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                file_paths.append((file_path, class_idx))\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_file, file_paths)\n",
    "\n",
    "    x, y = zip(*results)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Split and Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 299.0\n",
    "X_val = X_val / 299.0\n",
    "X_test = X_test / 299.0\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1174, 1024)\n",
      "(630, 1024)\n",
      "(294, 1024)\n",
      "(1174, 3)\n",
      "(630, 3)\n",
      "(294, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:59:55.967909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-29 17:59:55.968452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-29 17:59:55.968946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(VECTOR_LENGTH, 1))\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(128, return_sequences=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1024, 32)          4352      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024, 32)          128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024, 32)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1024, 64)          24832     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 1024, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024, 64)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163715 (639.51 KB)\n",
      "Trainable params: 162755 (635.76 KB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/tmp/90_LSTM_checkpoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 18:00:02.569289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-07-29 18:00:03.935294: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcfbc092700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 18:00:03.935330: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 18:00:03.935336: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 18:00:03.935341: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 18:00:03.941410: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 18:00:04.032396: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 2.5611 - accuracy: 0.7402\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63605, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 13s 137ms/step - loss: 2.5611 - accuracy: 0.7402 - val_loss: 2.5913 - val_accuracy: 0.6361 - lr: 0.0010\n",
      "Epoch 2/100\n",
      " 1/37 [..............................] - ETA: 3s - loss: 2.8521 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 2.2907 - accuracy: 0.7913\n",
      "Epoch 2: val_accuracy did not improve from 0.63605\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 2.2907 - accuracy: 0.7913 - val_loss: 2.4327 - val_accuracy: 0.6361 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 2.0749 - accuracy: 0.7998\n",
      "Epoch 3: val_accuracy did not improve from 0.63605\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 2.0749 - accuracy: 0.7998 - val_loss: 2.2986 - val_accuracy: 0.6361 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.8862 - accuracy: 0.8160\n",
      "Epoch 4: val_accuracy improved from 0.63605 to 0.64286, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 1.8862 - accuracy: 0.8160 - val_loss: 2.1448 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6938 - accuracy: 0.8211\n",
      "Epoch 5: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.6938 - accuracy: 0.8211 - val_loss: 2.1218 - val_accuracy: 0.1633 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5651 - accuracy: 0.8305\n",
      "Epoch 6: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.5651 - accuracy: 0.8305 - val_loss: 2.1697 - val_accuracy: 0.1633 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5374 - accuracy: 0.8109\n",
      "Epoch 7: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 1.5374 - accuracy: 0.8109 - val_loss: 2.0689 - val_accuracy: 0.1565 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.4183 - accuracy: 0.8194\n",
      "Epoch 8: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.4183 - accuracy: 0.8194 - val_loss: 1.9107 - val_accuracy: 0.1531 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.3205 - accuracy: 0.8186\n",
      "Epoch 9: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 1.3205 - accuracy: 0.8186 - val_loss: 2.3626 - val_accuracy: 0.1701 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.2665 - accuracy: 0.8262\n",
      "Epoch 10: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.2665 - accuracy: 0.8262 - val_loss: 2.4784 - val_accuracy: 0.1633 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.1921 - accuracy: 0.8296\n",
      "Epoch 11: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.1921 - accuracy: 0.8296 - val_loss: 1.6556 - val_accuracy: 0.6327 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.1538 - accuracy: 0.8313\n",
      "Epoch 12: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 1.1538 - accuracy: 0.8313 - val_loss: 2.0754 - val_accuracy: 0.3367 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.0652 - accuracy: 0.8339\n",
      "Epoch 13: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 1.0652 - accuracy: 0.8339 - val_loss: 1.3778 - val_accuracy: 0.6395 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9947 - accuracy: 0.8390\n",
      "Epoch 14: val_accuracy did not improve from 0.64286\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.9947 - accuracy: 0.8390 - val_loss: 2.3457 - val_accuracy: 0.2177 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9767 - accuracy: 0.8322\n",
      "Epoch 15: val_accuracy improved from 0.64286 to 0.77891, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.9767 - accuracy: 0.8322 - val_loss: 1.2276 - val_accuracy: 0.7789 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8908 - accuracy: 0.8501\n",
      "Epoch 16: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.8908 - accuracy: 0.8501 - val_loss: 3.2186 - val_accuracy: 0.1973 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9221 - accuracy: 0.8305\n",
      "Epoch 17: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.9221 - accuracy: 0.8305 - val_loss: 3.6146 - val_accuracy: 0.2007 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8762 - accuracy: 0.8177\n",
      "Epoch 18: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.8762 - accuracy: 0.8177 - val_loss: 2.3950 - val_accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.8143 - accuracy: 0.8458\n",
      "Epoch 19: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.8143 - accuracy: 0.8458 - val_loss: 3.1214 - val_accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7819 - accuracy: 0.8526\n",
      "Epoch 20: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.7819 - accuracy: 0.8526 - val_loss: 1.1116 - val_accuracy: 0.7347 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.8509\n",
      "Epoch 21: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.7175 - accuracy: 0.8509 - val_loss: 1.8929 - val_accuracy: 0.6565 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6985 - accuracy: 0.8526\n",
      "Epoch 22: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6985 - accuracy: 0.8526 - val_loss: 1.8133 - val_accuracy: 0.7177 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7078 - accuracy: 0.8552\n",
      "Epoch 23: val_accuracy did not improve from 0.77891\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.7078 - accuracy: 0.8552 - val_loss: 2.2225 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6796 - accuracy: 0.8552\n",
      "Epoch 24: val_accuracy improved from 0.77891 to 0.78912, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6796 - accuracy: 0.8552 - val_loss: 0.7707 - val_accuracy: 0.7891 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6327 - accuracy: 0.8612\n",
      "Epoch 25: val_accuracy improved from 0.78912 to 0.79932, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.6327 - accuracy: 0.8612 - val_loss: 1.6230 - val_accuracy: 0.7993 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.8595\n",
      "Epoch 26: val_accuracy did not improve from 0.79932\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.6514 - accuracy: 0.8595 - val_loss: 0.7364 - val_accuracy: 0.7925 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6159 - accuracy: 0.8603\n",
      "Epoch 27: val_accuracy did not improve from 0.79932\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.6159 - accuracy: 0.8603 - val_loss: 2.6050 - val_accuracy: 0.6633 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.8654\n",
      "Epoch 28: val_accuracy did not improve from 0.79932\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6045 - accuracy: 0.8654 - val_loss: 1.6193 - val_accuracy: 0.7211 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.8620\n",
      "Epoch 29: val_accuracy improved from 0.79932 to 0.80612, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5763 - accuracy: 0.8620 - val_loss: 1.2041 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.8595\n",
      "Epoch 30: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.5613 - accuracy: 0.8595 - val_loss: 1.9625 - val_accuracy: 0.2109 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5670 - accuracy: 0.8595\n",
      "Epoch 31: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.5670 - accuracy: 0.8595 - val_loss: 3.7300 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5542 - accuracy: 0.8637\n",
      "Epoch 32: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.5542 - accuracy: 0.8637 - val_loss: 2.8951 - val_accuracy: 0.6361 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.8279\n",
      "Epoch 33: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6022 - accuracy: 0.8279 - val_loss: 2.2835 - val_accuracy: 0.6565 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.8245\n",
      "Epoch 34: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.6009 - accuracy: 0.8245 - val_loss: 1.1146 - val_accuracy: 0.7959 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.8313\n",
      "Epoch 35: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5802 - accuracy: 0.8313 - val_loss: 1.1712 - val_accuracy: 0.7687 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5231 - accuracy: 0.8595\n",
      "Epoch 36: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.5231 - accuracy: 0.8595 - val_loss: 1.6400 - val_accuracy: 0.6939 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.8552\n",
      "Epoch 37: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5269 - accuracy: 0.8552 - val_loss: 1.5794 - val_accuracy: 0.6871 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5425 - accuracy: 0.8450\n",
      "Epoch 38: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.5425 - accuracy: 0.8450 - val_loss: 1.3579 - val_accuracy: 0.7041 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5246 - accuracy: 0.8629\n",
      "Epoch 39: val_accuracy did not improve from 0.80612\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5246 - accuracy: 0.8629 - val_loss: 0.6201 - val_accuracy: 0.8027 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.8569\n",
      "Epoch 40: val_accuracy improved from 0.80612 to 0.80952, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5210 - accuracy: 0.8569 - val_loss: 0.7603 - val_accuracy: 0.8095 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5006 - accuracy: 0.8569\n",
      "Epoch 41: val_accuracy improved from 0.80952 to 0.89116, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.5006 - accuracy: 0.8569 - val_loss: 0.4506 - val_accuracy: 0.8912 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4942 - accuracy: 0.8671\n",
      "Epoch 42: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4942 - accuracy: 0.8671 - val_loss: 0.5995 - val_accuracy: 0.8231 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4946 - accuracy: 0.8705\n",
      "Epoch 43: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4946 - accuracy: 0.8705 - val_loss: 0.6325 - val_accuracy: 0.8061 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.8509\n",
      "Epoch 44: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5169 - accuracy: 0.8509 - val_loss: 0.5259 - val_accuracy: 0.8333 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8663\n",
      "Epoch 45: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4843 - accuracy: 0.8663 - val_loss: 0.5598 - val_accuracy: 0.8435 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5148 - accuracy: 0.8569\n",
      "Epoch 46: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.5148 - accuracy: 0.8569 - val_loss: 0.4742 - val_accuracy: 0.8639 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.8552\n",
      "Epoch 47: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4996 - accuracy: 0.8552 - val_loss: 0.4764 - val_accuracy: 0.8707 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.8560\n",
      "Epoch 48: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4740 - accuracy: 0.8560 - val_loss: 0.4767 - val_accuracy: 0.8707 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4763 - accuracy: 0.8697\n",
      "Epoch 49: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4763 - accuracy: 0.8697 - val_loss: 1.4661 - val_accuracy: 0.3265 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4929 - accuracy: 0.8663\n",
      "Epoch 50: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4929 - accuracy: 0.8663 - val_loss: 1.0595 - val_accuracy: 0.3197 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4691 - accuracy: 0.8654\n",
      "Epoch 51: val_accuracy did not improve from 0.89116\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4691 - accuracy: 0.8654 - val_loss: 0.4219 - val_accuracy: 0.8707 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4898 - accuracy: 0.8569\n",
      "Epoch 52: val_accuracy improved from 0.89116 to 0.89456, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 108ms/step - loss: 0.4898 - accuracy: 0.8569 - val_loss: 0.4435 - val_accuracy: 0.8946 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4740 - accuracy: 0.8714\n",
      "Epoch 53: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4740 - accuracy: 0.8714 - val_loss: 0.7175 - val_accuracy: 0.8027 - lr: 1.2500e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4620 - accuracy: 0.8782\n",
      "Epoch 54: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.4620 - accuracy: 0.8782 - val_loss: 0.9785 - val_accuracy: 0.7483 - lr: 1.2500e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4614 - accuracy: 0.8629\n",
      "Epoch 55: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4614 - accuracy: 0.8629 - val_loss: 0.8236 - val_accuracy: 0.8129 - lr: 1.2500e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.8663\n",
      "Epoch 56: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4782 - accuracy: 0.8663 - val_loss: 0.4422 - val_accuracy: 0.8946 - lr: 1.2500e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8756\n",
      "Epoch 57: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4494 - accuracy: 0.8756 - val_loss: 0.9234 - val_accuracy: 0.3469 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.8637\n",
      "Epoch 58: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4806 - accuracy: 0.8637 - val_loss: 0.8769 - val_accuracy: 0.3469 - lr: 6.2500e-05\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4509 - accuracy: 0.8739\n",
      "Epoch 59: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4509 - accuracy: 0.8739 - val_loss: 1.1948 - val_accuracy: 0.3469 - lr: 6.2500e-05\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.8697\n",
      "Epoch 60: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4517 - accuracy: 0.8697 - val_loss: 0.8460 - val_accuracy: 0.8095 - lr: 6.2500e-05\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.8680\n",
      "Epoch 61: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4524 - accuracy: 0.8680 - val_loss: 1.1615 - val_accuracy: 0.3265 - lr: 6.2500e-05\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.8637\n",
      "Epoch 62: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4782 - accuracy: 0.8637 - val_loss: 0.8337 - val_accuracy: 0.8095 - lr: 3.1250e-05\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.8833\n",
      "Epoch 63: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4315 - accuracy: 0.8833 - val_loss: 0.4363 - val_accuracy: 0.8776 - lr: 3.1250e-05\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.8714\n",
      "Epoch 64: val_accuracy did not improve from 0.89456\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4478 - accuracy: 0.8714 - val_loss: 0.4017 - val_accuracy: 0.8741 - lr: 3.1250e-05\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4603 - accuracy: 0.8731\n",
      "Epoch 65: val_accuracy improved from 0.89456 to 0.89796, saving model to /tmp/90_LSTM_checkpoint.h5\n",
      "37/37 [==============================] - 4s 107ms/step - loss: 0.4603 - accuracy: 0.8731 - val_loss: 0.4062 - val_accuracy: 0.8980 - lr: 3.1250e-05\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4731 - accuracy: 0.8680\n",
      "Epoch 66: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4731 - accuracy: 0.8680 - val_loss: 0.4880 - val_accuracy: 0.8197 - lr: 3.1250e-05\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.8731\n",
      "Epoch 67: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4612 - accuracy: 0.8731 - val_loss: 0.4497 - val_accuracy: 0.8639 - lr: 3.1250e-05\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4412 - accuracy: 0.8773\n",
      "Epoch 68: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4412 - accuracy: 0.8773 - val_loss: 0.4456 - val_accuracy: 0.8639 - lr: 3.1250e-05\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8646\n",
      "Epoch 69: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4619 - accuracy: 0.8646 - val_loss: 1.5125 - val_accuracy: 0.3265 - lr: 3.1250e-05\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4679 - accuracy: 0.8603\n",
      "Epoch 70: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4679 - accuracy: 0.8603 - val_loss: 1.6040 - val_accuracy: 0.3265 - lr: 1.5625e-05\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.8671\n",
      "Epoch 71: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4839 - accuracy: 0.8671 - val_loss: 1.3516 - val_accuracy: 0.3469 - lr: 1.5625e-05\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4695 - accuracy: 0.8620\n",
      "Epoch 72: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4695 - accuracy: 0.8620 - val_loss: 1.0706 - val_accuracy: 0.3469 - lr: 1.5625e-05\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8595\n",
      "Epoch 73: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4461 - accuracy: 0.8595 - val_loss: 0.5612 - val_accuracy: 0.8197 - lr: 1.5625e-05\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4670 - accuracy: 0.8646\n",
      "Epoch 74: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4670 - accuracy: 0.8646 - val_loss: 0.4556 - val_accuracy: 0.8673 - lr: 1.5625e-05\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4496 - accuracy: 0.8714\n",
      "Epoch 75: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4496 - accuracy: 0.8714 - val_loss: 0.4395 - val_accuracy: 0.8673 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4520 - accuracy: 0.8654\n",
      "Epoch 76: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4520 - accuracy: 0.8654 - val_loss: 0.4245 - val_accuracy: 0.8980 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4512 - accuracy: 0.8790\n",
      "Epoch 77: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4512 - accuracy: 0.8790 - val_loss: 0.4184 - val_accuracy: 0.8912 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4552 - accuracy: 0.8680\n",
      "Epoch 78: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4552 - accuracy: 0.8680 - val_loss: 0.4195 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.8663\n",
      "Epoch 79: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4672 - accuracy: 0.8663 - val_loss: 0.4137 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4521 - accuracy: 0.8765\n",
      "Epoch 80: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4521 - accuracy: 0.8765 - val_loss: 0.4092 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8697\n",
      "Epoch 81: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4568 - accuracy: 0.8697 - val_loss: 0.4085 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.8790\n",
      "Epoch 82: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4470 - accuracy: 0.8790 - val_loss: 0.4072 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8731\n",
      "Epoch 83: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4494 - accuracy: 0.8731 - val_loss: 0.4022 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8705\n",
      "Epoch 84: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4461 - accuracy: 0.8705 - val_loss: 0.4015 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8765\n",
      "Epoch 85: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4513 - accuracy: 0.8765 - val_loss: 0.4022 - val_accuracy: 0.8946 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4512 - accuracy: 0.8637\n",
      "Epoch 86: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4512 - accuracy: 0.8637 - val_loss: 0.4235 - val_accuracy: 0.8469 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.8773\n",
      "Epoch 87: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4420 - accuracy: 0.8773 - val_loss: 0.4320 - val_accuracy: 0.8503 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4550 - accuracy: 0.8722\n",
      "Epoch 88: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4550 - accuracy: 0.8722 - val_loss: 0.4274 - val_accuracy: 0.8503 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4499 - accuracy: 0.8697\n",
      "Epoch 89: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4499 - accuracy: 0.8697 - val_loss: 0.4161 - val_accuracy: 0.8673 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.8756\n",
      "Epoch 90: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4396 - accuracy: 0.8756 - val_loss: 0.4485 - val_accuracy: 0.8503 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4779 - accuracy: 0.8612\n",
      "Epoch 91: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4779 - accuracy: 0.8612 - val_loss: 0.4585 - val_accuracy: 0.8537 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4587 - accuracy: 0.8697\n",
      "Epoch 92: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4587 - accuracy: 0.8697 - val_loss: 0.4504 - val_accuracy: 0.8707 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4405 - accuracy: 0.8807\n",
      "Epoch 93: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4405 - accuracy: 0.8807 - val_loss: 0.5176 - val_accuracy: 0.8435 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.8748\n",
      "Epoch 94: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4642 - accuracy: 0.8748 - val_loss: 0.5586 - val_accuracy: 0.8367 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.8748\n",
      "Epoch 95: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4477 - accuracy: 0.8748 - val_loss: 0.4593 - val_accuracy: 0.8741 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.8714\n",
      "Epoch 96: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4514 - accuracy: 0.8714 - val_loss: 0.4486 - val_accuracy: 0.8741 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4473 - accuracy: 0.8688\n",
      "Epoch 97: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4473 - accuracy: 0.8688 - val_loss: 0.4325 - val_accuracy: 0.8776 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4669 - accuracy: 0.8722\n",
      "Epoch 98: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 106ms/step - loss: 0.4669 - accuracy: 0.8722 - val_loss: 0.3978 - val_accuracy: 0.8707 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.8807\n",
      "Epoch 99: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4345 - accuracy: 0.8807 - val_loss: 0.3979 - val_accuracy: 0.8707 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4420 - accuracy: 0.8714\n",
      "Epoch 100: val_accuracy did not improve from 0.89796\n",
      "37/37 [==============================] - 4s 105ms/step - loss: 0.4420 - accuracy: 0.8714 - val_loss: 0.4234 - val_accuracy: 0.8707 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd1cb006380>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:43:22.684358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-31 22:43:22.684887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-31 22:43:22.685366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2024-07-31 22:43:25.622409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.4069 - accuracy: 0.8952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4069281816482544, 0.8952381014823914]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_model = load_model('/tmp/90_LSTM_checkpoint.h5')\n",
    "cp_model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 2s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = CLASSES[np.argmax(y_pred, axis = -1)]\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(X_test)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    data = X_test[idx]\n",
    "    ax = fig.add_subplot(1, n_to_show, i+1)\n",
    "    ax.plot(data)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.2, 'pred = ' + str(preds_single[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n",
    "    ax.text(0.5, -0.4, 'act = ' + str(actual_single[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9656    0.8915    0.9271       378\n",
      "           1     0.8875    0.9161    0.9016       155\n",
      "           2     0.7025    0.8763    0.7798        97\n",
      "\n",
      "    accuracy                         0.8952       630\n",
      "   macro avg     0.8519    0.8947    0.8695       630\n",
      "weighted avg     0.9059    0.8952    0.8981       630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_test_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        Benign  Sysrv  Xmrig\n",
      "Benign     337     17     24\n",
      "Sysrv        1    142     12\n",
      "Xmrig       11      1     85\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "class_labels = ['Benign', 'Sysrv', 'Xmrig']\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
