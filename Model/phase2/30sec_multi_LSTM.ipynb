{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:32:53.809811: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 22:32:53.859946: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 22:32:53.859986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 22:32:53.860019: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 22:32:53.868970: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, LSTM, BatchNormalization, Dropout, Dense, Add, Flatten\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Calls List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "syscalls = [\n",
    "\"sys_enter_llistxattr\",\n",
    "\"sys_enter_setgroups\",\n",
    "\"sys_enter_lremovexattr\",\n",
    "\"sys_enter_sethostname\",\n",
    "\"sys_enter_accept\",\n",
    "\"sys_enter_lseek\",\n",
    "\"sys_enter_setitimer\",\n",
    "\"sys_enter_accept4\",\n",
    "\"sys_enter_lsetxattr\",\n",
    "\"sys_enter_setns\",\n",
    "\"sys_enter_acct\",\n",
    "\"sys_enter_madvise\",\n",
    "\"sys_enter_setpgid\",\n",
    "\"sys_enter_add_key\",\n",
    "\"sys_enter_mbind\",\n",
    "\"sys_enter_setpriority\",\n",
    "\"sys_enter_adjtimex\",\n",
    "\"sys_enter_membarrier\",\n",
    "\"sys_enter_setregid\",\n",
    "\"sys_enter_personality\",\n",
    "\"sys_enter_memfd_create\",\n",
    "\"sys_enter_setresgid\",\n",
    "\"sys_enter_bind\",\n",
    "\"sys_enter_memfd_secret\",\n",
    "\"sys_enter_setresuid\",\n",
    "\"sys_enter_bpf\",\n",
    "\"sys_enter_migrate_pages\",\n",
    "\"sys_enter_setreuid\",\n",
    "\"sys_enter_brk\",\n",
    "\"sys_enter_mincore\",\n",
    "\"sys_enter_setrlimit\",\n",
    "\"sys_enter_capget\",\n",
    "\"sys_enter_mkdirat\",\n",
    "\"sys_enter_setsid\",\n",
    "\"sys_enter_capset\",\n",
    "\"sys_enter_mknodat\",\n",
    "\"sys_enter_setsockopt\",\n",
    "\"sys_enter_chdir\",\n",
    "\"sys_enter_mlock\",\n",
    "\"sys_enter_settimeofday\",\n",
    "\"sys_enter_chroot\",\n",
    "\"sys_enter_mlock2\",\n",
    "\"sys_enter_setuid\",\n",
    "\"sys_enter_clock_adjtime\",\n",
    "\"sys_enter_mlockall\",\n",
    "\"sys_enter_setxattr\",\n",
    "\"sys_enter_clock_getres\",\n",
    "\"sys_enter_mmap\",\n",
    "\"sys_enter_shmat\",\n",
    "\"sys_enter_clock_gettime\",\n",
    "\"sys_enter_mount\",\n",
    "\"sys_enter_shmctl\",\n",
    "\"sys_enter_clock_nanosleep\",\n",
    "\"sys_enter_mount_setattr\",\n",
    "\"sys_enter_shmdt\",\n",
    "\"sys_enter_clock_settime\",\n",
    "\"sys_enter_move_mount\",\n",
    "\"sys_enter_shmget\",\n",
    "\"sys_enter_clone\",\n",
    "\"sys_enter_move_pages\",\n",
    "\"sys_enter_shutdown\",\n",
    "\"sys_enter_clone3\",\n",
    "\"sys_enter_mprotect\",\n",
    "\"sys_enter_sigaltstack\",\n",
    "\"sys_enter_close\",\n",
    "\"sys_enter_mq_getsetattr\",\n",
    "\"sys_enter_signalfd4\",\n",
    "\"sys_enter_close_range\",\n",
    "\"sys_enter_mq_notify\",\n",
    "\"sys_enter_socket\",\n",
    "\"sys_enter_connect\",\n",
    "\"sys_enter_mq_open\",\n",
    "\"sys_enter_socketpair\",\n",
    "\"sys_enter_copy_file_range\",\n",
    "\"sys_enter_mq_timedreceive\",\n",
    "\"sys_enter_splice\",\n",
    "\"sys_enter_delete_module\",\n",
    "\"sys_enter_mq_timedsend\",\n",
    "\"sys_enter_statfs\",\n",
    "\"sys_enter_dup\",\n",
    "\"sys_enter_mq_unlink\",\n",
    "\"sys_enter_statx\",\n",
    "\"sys_enter_dup3\",\n",
    "\"sys_enter_mremap\",\n",
    "\"sys_enter_swapoff\",\n",
    "\"sys_enter_epoll_create1\",\n",
    "\"sys_enter_msgctl\",\n",
    "\"sys_enter_swapon\",\n",
    "\"sys_enter_epoll_ctl\",\n",
    "\"sys_enter_msgget\",\n",
    "\"sys_enter_symlinkat\",\n",
    "\"sys_enter_epoll_pwait\",\n",
    "\"sys_enter_msgrcv\",\n",
    "\"sys_enter_sync\",\n",
    "\"sys_enter_epoll_pwait2\",\n",
    "\"sys_enter_msgsnd\",\n",
    "\"sys_enter_sync_file_range\",\n",
    "\"sys_enter_eventfd2\",\n",
    "\"sys_enter_msync\",\n",
    "\"sys_enter_syncfs\",\n",
    "\"sys_enter_execve\",\n",
    "\"sys_enter_munlock\",\n",
    "\"sys_enter_sysinfo\",\n",
    "\"sys_enter_execveat\",\n",
    "\"sys_enter_munlockall\",\n",
    "\"sys_enter_syslog\",\n",
    "\"sys_enter_exit\",\n",
    "\"sys_enter_munmap\",\n",
    "\"sys_enter_tee\",\n",
    "\"sys_enter_exit_group\",\n",
    "\"sys_enter_name_to_handle_at\",\n",
    "\"sys_enter_tgkill\",\n",
    "\"sys_enter_faccessat\",\n",
    "\"sys_enter_nanosleep\",\n",
    "\"sys_enter_timer_create\",\n",
    "\"sys_enter_faccessat2\",\n",
    "\"sys_enter_newfstat\",\n",
    "\"sys_enter_timer_delete\",\n",
    "\"sys_enter_fadvise64\",\n",
    "\"sys_enter_newfstatat\",\n",
    "\"sys_enter_timer_getoverrun\",\n",
    "\"sys_enter_fallocate\",\n",
    "\"sys_enter_newuname\",\n",
    "\"sys_enter_timer_gettime\",\n",
    "\"sys_enter_fanotify_init\",\n",
    "\"sys_enter_open_by_handle_at\",\n",
    "\"sys_enter_timer_settime\",\n",
    "\"sys_enter_fanotify_mark\",\n",
    "\"sys_enter_open_tree\",\n",
    "\"sys_enter_timerfd_create\",\n",
    "\"sys_enter_fchdir\",\n",
    "\"sys_enter_openat\",\n",
    "\"sys_enter_timerfd_gettime\",\n",
    "\"sys_enter_fchmod\",\n",
    "\"sys_enter_openat2\",\n",
    "\"sys_enter_timerfd_settime\",\n",
    "\"sys_enter_fchmodat\",\n",
    "\"sys_enter_perf_event_open\",\n",
    "\"sys_enter_times\",\n",
    "\"sys_enter_fchown\",\n",
    "\"sys_enter_pidfd_getfd\",\n",
    "\"sys_enter_tkill\",\n",
    "\"sys_enter_fchownat\",\n",
    "\"sys_enter_pidfd_open\",\n",
    "\"sys_enter_truncate\",\n",
    "\"sys_enter_fcntl\",\n",
    "\"sys_enter_pidfd_send_signal\",\n",
    "\"sys_enter_umask\",\n",
    "\"sys_enter_fdatasync\",\n",
    "\"sys_enter_pipe2\",\n",
    "\"sys_enter_umount\",\n",
    "\"sys_enter_fgetxattr\",\n",
    "\"sys_enter_pivot_root\",\n",
    "\"sys_enter_unlinkat\",\n",
    "\"sys_enter_finit_module\",\n",
    "\"sys_enter_ppoll\",\n",
    "\"sys_enter_unshare\",\n",
    "\"sys_enter_flistxattr\",\n",
    "\"sys_enter_prctl\",\n",
    "\"sys_enter_userfaultfd\",\n",
    "\"sys_enter_flock\",\n",
    "\"sys_enter_pread64\",\n",
    "\"sys_enter_utimensat\",\n",
    "\"sys_enter_fremovexattr\",\n",
    "\"sys_enter_preadv\",\n",
    "\"sys_enter_vhangup\",\n",
    "\"sys_enter_fsconfig\",\n",
    "\"sys_enter_preadv2\",\n",
    "\"sys_enter_vmsplice\",\n",
    "\"sys_enter_fsetxattr\",\n",
    "\"sys_enter_prlimit64\",\n",
    "\"sys_enter_wait4\",\n",
    "\"sys_enter_fsmount\",\n",
    "\"sys_enter_process_madvise\",\n",
    "\"sys_enter_waitid\",\n",
    "\"sys_enter_fsopen\",\n",
    "\"sys_enter_process_mrelease\",\n",
    "\"sys_enter_write\",\n",
    "\"sys_enter_fspick\",\n",
    "\"sys_enter_process_vm_readv\",\n",
    "\"sys_enter_writev\",\n",
    "\"sys_enter_fstatfs\",\n",
    "\"sys_enter_process_vm_writev\",\n",
    "\"sys_enter_fsync\",\n",
    "\"sys_enter_pselect6\",\n",
    "\"sys_enter_ftruncate\",\n",
    "\"sys_enter_ptrace\",\n",
    "\"sys_enter_futex\",\n",
    "\"sys_enter_pwrite64\",\n",
    "\"sys_enter_get_mempolicy\",\n",
    "\"sys_enter_pwritev\",\n",
    "\"sys_enter_get_robust_list\",\n",
    "\"sys_enter_pwritev2\",\n",
    "\"sys_enter_getcpu\",\n",
    "\"sys_enter_quotactl\",\n",
    "\"sys_enter_getcwd\",\n",
    "\"sys_enter_quotactl_fd\",\n",
    "\"sys_enter_getdents64\",\n",
    "\"sys_enter_read\",\n",
    "\"sys_enter_getegid\",\n",
    "\"sys_enter_readahead\",\n",
    "\"sys_enter_geteuid\",\n",
    "\"sys_enter_readlinkat\",\n",
    "\"sys_enter_getgid\",\n",
    "\"sys_enter_readv\",\n",
    "\"sys_enter_getgroups\",\n",
    "\"sys_enter_reboot\",\n",
    "\"sys_enter_getitimer\",\n",
    "\"sys_enter_recvfrom\",\n",
    "\"sys_enter_getpeername\",\n",
    "\"sys_enter_recvmmsg\",\n",
    "\"sys_enter_getpgid\",\n",
    "\"sys_enter_recvmsg\",\n",
    "\"sys_enter_getpid\",\n",
    "\"sys_enter_remap_file_pages\",\n",
    "\"sys_enter_getppid\",\n",
    "\"sys_enter_removexattr\",\n",
    "\"sys_enter_getpriority\",\n",
    "\"sys_enter_renameat\",\n",
    "\"sys_enter_getrandom\",\n",
    "\"sys_enter_renameat2\",\n",
    "\"sys_enter_getresgid\",\n",
    "\"sys_enter_request_key\",\n",
    "\"sys_enter_getresuid\",\n",
    "\"sys_enter_restart_syscall\",\n",
    "\"sys_enter_getrlimit\",\n",
    "\"sys_enter_rseq\",\n",
    "\"sys_enter_getrusage\",\n",
    "\"sys_enter_rt_sigaction\",\n",
    "\"sys_enter_getsid\",\n",
    "\"sys_enter_rt_sigpending\",\n",
    "\"sys_enter_getsockname\",\n",
    "\"sys_enter_rt_sigprocmask\",\n",
    "\"sys_enter_getsockopt\",\n",
    "\"sys_enter_rt_sigqueueinfo\",\n",
    "\"sys_enter_gettid\",\n",
    "\"sys_enter_rt_sigreturn\",\n",
    "\"sys_enter_gettimeofday\",\n",
    "\"sys_enter_rt_sigsuspend\",\n",
    "\"sys_enter_getuid\",\n",
    "\"sys_enter_rt_sigtimedwait\",\n",
    "\"sys_enter_getxattr\",\n",
    "\"sys_enter_rt_tgsigqueueinfo\",\n",
    "\"sys_enter_init_module\",\n",
    "\"sys_enter_sched_get_priority_max\",\n",
    "\"sys_enter_inotify_add_watch\",\n",
    "\"sys_enter_sched_get_priority_min\",\n",
    "\"sys_enter_inotify_init1\",\n",
    "\"sys_enter_sched_getaffinity\",\n",
    "\"sys_enter_inotify_rm_watch\",\n",
    "\"sys_enter_sched_getattr\",\n",
    "\"sys_enter_io_cancel\",\n",
    "\"sys_enter_sched_getparam\",\n",
    "\"sys_enter_io_destroy\",\n",
    "\"sys_enter_sched_getscheduler\",\n",
    "\"sys_enter_io_getevents\",\n",
    "\"sys_enter_sched_rr_get_interval\",\n",
    "\"sys_enter_io_pgetevents\",\n",
    "\"sys_enter_sched_setaffinity\",\n",
    "\"sys_enter_io_setup\",\n",
    "\"sys_enter_sched_setattr\",\n",
    "\"sys_enter_io_submit\",\n",
    "\"sys_enter_sched_setparam\",\n",
    "\"sys_enter_io_uring_enter\",\n",
    "\"sys_enter_sched_setscheduler\",\n",
    "\"sys_enter_io_uring_register\",\n",
    "\"sys_enter_sched_yield\",\n",
    "\"sys_enter_io_uring_setup\",\n",
    "\"sys_enter_seccomp\",\n",
    "\"sys_enter_ioctl\",\n",
    "\"sys_enter_semctl\",\n",
    "\"sys_enter_ioprio_get\",\n",
    "\"sys_enter_semget\",\n",
    "\"sys_enter_ioprio_set\",\n",
    "\"sys_enter_semop\",\n",
    "\"sys_enter_kcmp\",\n",
    "\"sys_enter_semtimedop\",\n",
    "\"sys_enter_kexec_file_load\",\n",
    "\"sys_enter_sendfile64\",\n",
    "\"sys_enter_kexec_load\",\n",
    "\"sys_enter_sendmmsg\",\n",
    "\"sys_enter_keyctl\",\n",
    "\"sys_enter_sendmsg\",\n",
    "\"sys_enter_kill\",\n",
    "\"sys_enter_sendto\",\n",
    "\"sys_enter_landlock_add_rule\",\n",
    "\"sys_enter_set_mempolicy\",\n",
    "\"sys_enter_landlock_create_ruleset\",\n",
    "\"sys_enter_set_robust_list\",\n",
    "\"sys_enter_landlock_restrict_self\",\n",
    "\"sys_enter_set_tid_address\",\n",
    "\"sys_enter_lgetxattr\",\n",
    "\"sys_enter_setdomainname\",\n",
    "\"sys_enter_linkat\",\n",
    "\"sys_enter_setfsgid\",\n",
    "\"sys_enter_listen\",\n",
    "\"sys_enter_setfsuid\",\n",
    "\"sys_enter_listxattr\",\n",
    "\"sys_enter_setgid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV from Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "CLASSES = np.array(['benign', 'sysrv', 'xmrig'])\n",
    "DATASET_DIR = \"raw_data/\"\n",
    "VECTOR_LENGTH = 32 * 32\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(syscalls)\n",
    "\n",
    "def csvToVector(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    data_encoded = label_encoder.fit_transform(data['SYSTEM_CALL'])\n",
    "    vector = np.zeros(VECTOR_LENGTH, dtype=np.uint8)\n",
    "    syscall_nums = min(len(data_encoded), VECTOR_LENGTH)\n",
    "    vector[:syscall_nums] = data_encoded[:syscall_nums]\n",
    "\n",
    "    return vector\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, class_idx = args\n",
    "    vector = csvToVector(file_path)\n",
    "    return vector, class_idx\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    classes = [\"0/30sec_0\", \"1/30sec_1\", \"2/30sec_2\"]\n",
    "\n",
    "    file_paths = []\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                file_paths.append((file_path, class_idx))\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_file, file_paths)\n",
    "\n",
    "    x, y = zip(*results)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Split and Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 299.0\n",
    "X_val = X_val / 299.0\n",
    "X_test = X_test / 299.0\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2072, 1024)\n",
      "(1110, 1024)\n",
      "(518, 1024)\n",
      "(2072, 3)\n",
      "(1110, 3)\n",
      "(518, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:24:15.053781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-29 17:24:15.054317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-29 17:24:15.054811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(VECTOR_LENGTH, 1))\n",
    "\n",
    "x = LSTM(32, return_sequences=True)(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(64, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(128, return_sequences=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output_layer = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1024, 32)          4352      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024, 32)          128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024, 32)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1024, 64)          24832     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 1024, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024, 64)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               98816     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163715 (639.51 KB)\n",
      "Trainable params: 162755 (635.76 KB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/tmp/30_LSTM_checkpoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:24:21.637091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-07-29 17:24:23.003122: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc63402aab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 17:24:23.003158: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:24:23.003164: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:24:23.003169: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:24:23.008941: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 17:24:23.099886: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - ETA: 0s - loss: 2.3405 - accuracy: 0.7915\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67375, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 15s 124ms/step - loss: 2.3405 - accuracy: 0.7915 - val_loss: 2.5127 - val_accuracy: 0.6737 - lr: 0.0010\n",
      "Epoch 2/100\n",
      " 1/65 [..............................] - ETA: 6s - loss: 1.9513 - accuracy: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - ETA: 0s - loss: 1.9480 - accuracy: 0.8287\n",
      "Epoch 2: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 1.9480 - accuracy: 0.8287 - val_loss: 2.4837 - val_accuracy: 0.1274 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.7173 - accuracy: 0.8238\n",
      "Epoch 3: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 1.7173 - accuracy: 0.8238 - val_loss: 2.4525 - val_accuracy: 0.1274 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.5043 - accuracy: 0.8292\n",
      "Epoch 4: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 1.5043 - accuracy: 0.8292 - val_loss: 2.3331 - val_accuracy: 0.1274 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.3823 - accuracy: 0.8340\n",
      "Epoch 5: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 1.3823 - accuracy: 0.8340 - val_loss: 1.7287 - val_accuracy: 0.5811 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.2215 - accuracy: 0.8509\n",
      "Epoch 6: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 1.2215 - accuracy: 0.8509 - val_loss: 2.3510 - val_accuracy: 0.1274 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.8547\n",
      "Epoch 7: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 1.0925 - accuracy: 0.8547 - val_loss: 1.5692 - val_accuracy: 0.5849 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.9786 - accuracy: 0.8514\n",
      "Epoch 8: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.9786 - accuracy: 0.8514 - val_loss: 1.9008 - val_accuracy: 0.5946 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.8882 - accuracy: 0.8615\n",
      "Epoch 9: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 105ms/step - loss: 0.8882 - accuracy: 0.8615 - val_loss: 2.1105 - val_accuracy: 0.5985 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.8345 - accuracy: 0.8600\n",
      "Epoch 10: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.8345 - accuracy: 0.8600 - val_loss: 2.9986 - val_accuracy: 0.6004 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.7325 - accuracy: 0.8687\n",
      "Epoch 11: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.7325 - accuracy: 0.8687 - val_loss: 3.4610 - val_accuracy: 0.5985 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.8682\n",
      "Epoch 12: val_accuracy did not improve from 0.67375\n",
      "65/65 [==============================] - 7s 108ms/step - loss: 0.6921 - accuracy: 0.8682 - val_loss: 2.3467 - val_accuracy: 0.6023 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6462 - accuracy: 0.8639\n",
      "Epoch 13: val_accuracy improved from 0.67375 to 0.77606, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 9s 139ms/step - loss: 0.6462 - accuracy: 0.8639 - val_loss: 1.0354 - val_accuracy: 0.7761 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6140 - accuracy: 0.8760\n",
      "Epoch 14: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 9s 138ms/step - loss: 0.6140 - accuracy: 0.8760 - val_loss: 2.2461 - val_accuracy: 0.6351 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.8760\n",
      "Epoch 15: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 10s 157ms/step - loss: 0.5944 - accuracy: 0.8760 - val_loss: 2.4393 - val_accuracy: 0.6004 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5655 - accuracy: 0.8851\n",
      "Epoch 16: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 8s 116ms/step - loss: 0.5655 - accuracy: 0.8851 - val_loss: 2.0717 - val_accuracy: 0.6313 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5938 - accuracy: 0.8591\n",
      "Epoch 17: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 7s 109ms/step - loss: 0.5938 - accuracy: 0.8591 - val_loss: 1.8066 - val_accuracy: 0.2780 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.8813\n",
      "Epoch 18: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.5294 - accuracy: 0.8813 - val_loss: 1.8771 - val_accuracy: 0.2780 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.8885\n",
      "Epoch 19: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.5013 - accuracy: 0.8885 - val_loss: 2.0376 - val_accuracy: 0.2819 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.8847\n",
      "Epoch 20: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.5210 - accuracy: 0.8847 - val_loss: 1.0132 - val_accuracy: 0.2876 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.8866\n",
      "Epoch 21: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.4922 - accuracy: 0.8866 - val_loss: 1.6969 - val_accuracy: 0.3263 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.8866\n",
      "Epoch 22: val_accuracy did not improve from 0.77606\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.5035 - accuracy: 0.8866 - val_loss: 1.0292 - val_accuracy: 0.2876 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.8938\n",
      "Epoch 23: val_accuracy improved from 0.77606 to 0.81660, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.4642 - accuracy: 0.8938 - val_loss: 0.5571 - val_accuracy: 0.8166 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4474 - accuracy: 0.8986\n",
      "Epoch 24: val_accuracy did not improve from 0.81660\n",
      "65/65 [==============================] - 7s 111ms/step - loss: 0.4474 - accuracy: 0.8986 - val_loss: 1.4704 - val_accuracy: 0.3629 - lr: 2.5000e-04\n",
      "Epoch 25/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4633 - accuracy: 0.8904\n",
      "Epoch 25: val_accuracy improved from 0.81660 to 0.83012, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 9s 140ms/step - loss: 0.4633 - accuracy: 0.8904 - val_loss: 0.5786 - val_accuracy: 0.8301 - lr: 2.5000e-04\n",
      "Epoch 26/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.8943\n",
      "Epoch 26: val_accuracy did not improve from 0.83012\n",
      "65/65 [==============================] - 9s 132ms/step - loss: 0.4482 - accuracy: 0.8943 - val_loss: 0.5975 - val_accuracy: 0.8205 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8842\n",
      "Epoch 27: val_accuracy improved from 0.83012 to 0.84556, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 10s 155ms/step - loss: 0.4469 - accuracy: 0.8842 - val_loss: 0.6169 - val_accuracy: 0.8456 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.8900\n",
      "Epoch 28: val_accuracy improved from 0.84556 to 0.87066, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 8s 124ms/step - loss: 0.4258 - accuracy: 0.8900 - val_loss: 0.5661 - val_accuracy: 0.8707 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4059 - accuracy: 0.9035\n",
      "Epoch 29: val_accuracy did not improve from 0.87066\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.4059 - accuracy: 0.9035 - val_loss: 0.6396 - val_accuracy: 0.7722 - lr: 1.2500e-04\n",
      "Epoch 30/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4531 - accuracy: 0.8866\n",
      "Epoch 30: val_accuracy did not improve from 0.87066\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.4531 - accuracy: 0.8866 - val_loss: 0.5482 - val_accuracy: 0.8185 - lr: 1.2500e-04\n",
      "Epoch 31/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8967\n",
      "Epoch 31: val_accuracy improved from 0.87066 to 0.88803, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.4097 - accuracy: 0.8967 - val_loss: 0.3963 - val_accuracy: 0.8880 - lr: 1.2500e-04\n",
      "Epoch 32/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4093 - accuracy: 0.8943\n",
      "Epoch 32: val_accuracy did not improve from 0.88803\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.4093 - accuracy: 0.8943 - val_loss: 0.9867 - val_accuracy: 0.3166 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.9001\n",
      "Epoch 33: val_accuracy did not improve from 0.88803\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.4046 - accuracy: 0.9001 - val_loss: 1.0963 - val_accuracy: 0.3668 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4080 - accuracy: 0.8962\n",
      "Epoch 34: val_accuracy did not improve from 0.88803\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.4080 - accuracy: 0.8962 - val_loss: 2.1236 - val_accuracy: 0.2780 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3901 - accuracy: 0.8996\n",
      "Epoch 35: val_accuracy did not improve from 0.88803\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3901 - accuracy: 0.8996 - val_loss: 0.7869 - val_accuracy: 0.8764 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8982\n",
      "Epoch 36: val_accuracy did not improve from 0.88803\n",
      "65/65 [==============================] - 7s 108ms/step - loss: 0.3997 - accuracy: 0.8982 - val_loss: 1.7153 - val_accuracy: 0.3263 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.9015\n",
      "Epoch 37: val_accuracy did not improve from 0.88803\n",
      "65/65 [==============================] - 9s 134ms/step - loss: 0.3904 - accuracy: 0.9015 - val_loss: 1.8342 - val_accuracy: 0.3224 - lr: 6.2500e-05\n",
      "Epoch 38/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3772 - accuracy: 0.9035\n",
      "Epoch 38: val_accuracy improved from 0.88803 to 0.89189, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 112ms/step - loss: 0.3772 - accuracy: 0.9035 - val_loss: 0.5021 - val_accuracy: 0.8919 - lr: 6.2500e-05\n",
      "Epoch 39/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.9069\n",
      "Epoch 39: val_accuracy did not improve from 0.89189\n",
      "65/65 [==============================] - 9s 139ms/step - loss: 0.3844 - accuracy: 0.9069 - val_loss: 0.3664 - val_accuracy: 0.8900 - lr: 6.2500e-05\n",
      "Epoch 40/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.9073\n",
      "Epoch 40: val_accuracy did not improve from 0.89189\n",
      "65/65 [==============================] - 8s 118ms/step - loss: 0.3695 - accuracy: 0.9073 - val_loss: 0.5593 - val_accuracy: 0.8784 - lr: 6.2500e-05\n",
      "Epoch 41/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.9049\n",
      "Epoch 41: val_accuracy did not improve from 0.89189\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3749 - accuracy: 0.9049 - val_loss: 1.9194 - val_accuracy: 0.3243 - lr: 6.2500e-05\n",
      "Epoch 42/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3906 - accuracy: 0.8962\n",
      "Epoch 42: val_accuracy did not improve from 0.89189\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3906 - accuracy: 0.8962 - val_loss: 0.9990 - val_accuracy: 0.3552 - lr: 6.2500e-05\n",
      "Epoch 43/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.8938\n",
      "Epoch 43: val_accuracy did not improve from 0.89189\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3803 - accuracy: 0.8938 - val_loss: 0.4021 - val_accuracy: 0.8687 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3829 - accuracy: 0.9020\n",
      "Epoch 44: val_accuracy improved from 0.89189 to 0.90154, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3829 - accuracy: 0.9020 - val_loss: 0.3611 - val_accuracy: 0.9015 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3722 - accuracy: 0.9025\n",
      "Epoch 45: val_accuracy did not improve from 0.90154\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3722 - accuracy: 0.9025 - val_loss: 0.4526 - val_accuracy: 0.8919 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3843 - accuracy: 0.8996\n",
      "Epoch 46: val_accuracy did not improve from 0.90154\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3843 - accuracy: 0.8996 - val_loss: 0.4164 - val_accuracy: 0.8803 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3713 - accuracy: 0.9064\n",
      "Epoch 47: val_accuracy did not improve from 0.90154\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3713 - accuracy: 0.9064 - val_loss: 0.4741 - val_accuracy: 0.8919 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.9040\n",
      "Epoch 48: val_accuracy did not improve from 0.90154\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3642 - accuracy: 0.9040 - val_loss: 0.5911 - val_accuracy: 0.8764 - lr: 6.2500e-05\n",
      "Epoch 49/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3706 - accuracy: 0.9025\n",
      "Epoch 49: val_accuracy did not improve from 0.90154\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3706 - accuracy: 0.9025 - val_loss: 0.6728 - val_accuracy: 0.8822 - lr: 6.2500e-05\n",
      "Epoch 50/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3696 - accuracy: 0.8982\n",
      "Epoch 50: val_accuracy did not improve from 0.90154\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3696 - accuracy: 0.8982 - val_loss: 0.4657 - val_accuracy: 0.8900 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3608 - accuracy: 0.9030\n",
      "Epoch 51: val_accuracy improved from 0.90154 to 0.90927, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3608 - accuracy: 0.9030 - val_loss: 0.3659 - val_accuracy: 0.9093 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.9069\n",
      "Epoch 52: val_accuracy did not improve from 0.90927\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3497 - accuracy: 0.9069 - val_loss: 0.3969 - val_accuracy: 0.8784 - lr: 3.1250e-05\n",
      "Epoch 53/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.9059\n",
      "Epoch 53: val_accuracy did not improve from 0.90927\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3656 - accuracy: 0.9059 - val_loss: 0.3561 - val_accuracy: 0.9093 - lr: 3.1250e-05\n",
      "Epoch 54/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3601 - accuracy: 0.9069\n",
      "Epoch 54: val_accuracy did not improve from 0.90927\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3601 - accuracy: 0.9069 - val_loss: 0.5543 - val_accuracy: 0.8842 - lr: 3.1250e-05\n",
      "Epoch 55/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.9064\n",
      "Epoch 55: val_accuracy did not improve from 0.90927\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3519 - accuracy: 0.9064 - val_loss: 0.4193 - val_accuracy: 0.8938 - lr: 3.1250e-05\n",
      "Epoch 56/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.9030\n",
      "Epoch 56: val_accuracy did not improve from 0.90927\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3612 - accuracy: 0.9030 - val_loss: 0.9343 - val_accuracy: 0.3378 - lr: 3.1250e-05\n",
      "Epoch 57/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3580 - accuracy: 0.9044\n",
      "Epoch 57: val_accuracy improved from 0.90927 to 0.91313, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3580 - accuracy: 0.9044 - val_loss: 0.3928 - val_accuracy: 0.9131 - lr: 3.1250e-05\n",
      "Epoch 58/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.9025\n",
      "Epoch 58: val_accuracy did not improve from 0.91313\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3648 - accuracy: 0.9025 - val_loss: 0.3444 - val_accuracy: 0.8938 - lr: 3.1250e-05\n",
      "Epoch 59/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.8986\n",
      "Epoch 59: val_accuracy did not improve from 0.91313\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3671 - accuracy: 0.8986 - val_loss: 0.3437 - val_accuracy: 0.8938 - lr: 3.1250e-05\n",
      "Epoch 60/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3588 - accuracy: 0.9107\n",
      "Epoch 60: val_accuracy did not improve from 0.91313\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3588 - accuracy: 0.9107 - val_loss: 0.4142 - val_accuracy: 0.8764 - lr: 3.1250e-05\n",
      "Epoch 61/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.9069\n",
      "Epoch 61: val_accuracy did not improve from 0.91313\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3566 - accuracy: 0.9069 - val_loss: 0.3633 - val_accuracy: 0.9112 - lr: 3.1250e-05\n",
      "Epoch 62/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.9088\n",
      "Epoch 62: val_accuracy did not improve from 0.91313\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3563 - accuracy: 0.9088 - val_loss: 0.3385 - val_accuracy: 0.9093 - lr: 3.1250e-05\n",
      "Epoch 63/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3615 - accuracy: 0.9011\n",
      "Epoch 63: val_accuracy improved from 0.91313 to 0.91506, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3615 - accuracy: 0.9011 - val_loss: 0.3343 - val_accuracy: 0.9151 - lr: 3.1250e-05\n",
      "Epoch 64/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.9083\n",
      "Epoch 64: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3553 - accuracy: 0.9083 - val_loss: 0.3346 - val_accuracy: 0.8996 - lr: 3.1250e-05\n",
      "Epoch 65/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.9117\n",
      "Epoch 65: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3456 - accuracy: 0.9117 - val_loss: 0.3351 - val_accuracy: 0.9112 - lr: 3.1250e-05\n",
      "Epoch 66/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3453 - accuracy: 0.9107\n",
      "Epoch 66: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3453 - accuracy: 0.9107 - val_loss: 0.3347 - val_accuracy: 0.9131 - lr: 3.1250e-05\n",
      "Epoch 67/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.9097\n",
      "Epoch 67: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3460 - accuracy: 0.9097 - val_loss: 0.3306 - val_accuracy: 0.9131 - lr: 3.1250e-05\n",
      "Epoch 68/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.9049\n",
      "Epoch 68: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3531 - accuracy: 0.9049 - val_loss: 0.3368 - val_accuracy: 0.8919 - lr: 3.1250e-05\n",
      "Epoch 69/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3525 - accuracy: 0.9059\n",
      "Epoch 69: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3525 - accuracy: 0.9059 - val_loss: 0.4108 - val_accuracy: 0.8880 - lr: 3.1250e-05\n",
      "Epoch 70/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.9088\n",
      "Epoch 70: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3464 - accuracy: 0.9088 - val_loss: 0.3880 - val_accuracy: 0.8958 - lr: 3.1250e-05\n",
      "Epoch 71/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3543 - accuracy: 0.9040\n",
      "Epoch 71: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3543 - accuracy: 0.9040 - val_loss: 0.4363 - val_accuracy: 0.8764 - lr: 3.1250e-05\n",
      "Epoch 72/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.9083\n",
      "Epoch 72: val_accuracy did not improve from 0.91506\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3329 - accuracy: 0.9083 - val_loss: 0.3331 - val_accuracy: 0.8996 - lr: 3.1250e-05\n",
      "Epoch 73/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3385 - accuracy: 0.9122\n",
      "Epoch 73: val_accuracy improved from 0.91506 to 0.91699, saving model to /tmp/30_LSTM_checkpoint.h5\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3385 - accuracy: 0.9122 - val_loss: 0.3236 - val_accuracy: 0.9170 - lr: 1.5625e-05\n",
      "Epoch 74/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.9078\n",
      "Epoch 74: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3501 - accuracy: 0.9078 - val_loss: 0.3260 - val_accuracy: 0.9112 - lr: 1.5625e-05\n",
      "Epoch 75/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3339 - accuracy: 0.9069\n",
      "Epoch 75: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3339 - accuracy: 0.9069 - val_loss: 0.3722 - val_accuracy: 0.8861 - lr: 1.5625e-05\n",
      "Epoch 76/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.9112\n",
      "Epoch 76: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3316 - accuracy: 0.9112 - val_loss: 0.3650 - val_accuracy: 0.8900 - lr: 1.5625e-05\n",
      "Epoch 77/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.9069\n",
      "Epoch 77: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3465 - accuracy: 0.9069 - val_loss: 0.3245 - val_accuracy: 0.9112 - lr: 1.5625e-05\n",
      "Epoch 78/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.9064\n",
      "Epoch 78: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3397 - accuracy: 0.9064 - val_loss: 0.3252 - val_accuracy: 0.9151 - lr: 1.5625e-05\n",
      "Epoch 79/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3371 - accuracy: 0.9093\n",
      "Epoch 79: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3371 - accuracy: 0.9093 - val_loss: 0.3390 - val_accuracy: 0.9131 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.9011\n",
      "Epoch 80: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3471 - accuracy: 0.9011 - val_loss: 0.3485 - val_accuracy: 0.9131 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.9112\n",
      "Epoch 81: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3332 - accuracy: 0.9112 - val_loss: 0.3299 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3468 - accuracy: 0.9122\n",
      "Epoch 82: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 8s 116ms/step - loss: 0.3468 - accuracy: 0.9122 - val_loss: 0.3196 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.9030\n",
      "Epoch 83: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 8s 129ms/step - loss: 0.3434 - accuracy: 0.9030 - val_loss: 0.3635 - val_accuracy: 0.9131 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3377 - accuracy: 0.9069\n",
      "Epoch 84: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 8s 120ms/step - loss: 0.3377 - accuracy: 0.9069 - val_loss: 0.3306 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.9102\n",
      "Epoch 85: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 113ms/step - loss: 0.3343 - accuracy: 0.9102 - val_loss: 0.3231 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.9131\n",
      "Epoch 86: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 111ms/step - loss: 0.3441 - accuracy: 0.9131 - val_loss: 0.3184 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.9083\n",
      "Epoch 87: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 112ms/step - loss: 0.3411 - accuracy: 0.9083 - val_loss: 0.3384 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.9093\n",
      "Epoch 88: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 110ms/step - loss: 0.3370 - accuracy: 0.9093 - val_loss: 0.3208 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3361 - accuracy: 0.9136\n",
      "Epoch 89: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3361 - accuracy: 0.9136 - val_loss: 0.3210 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3389 - accuracy: 0.9040\n",
      "Epoch 90: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 108ms/step - loss: 0.3389 - accuracy: 0.9040 - val_loss: 0.3196 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.9122\n",
      "Epoch 91: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 8s 121ms/step - loss: 0.3348 - accuracy: 0.9122 - val_loss: 0.3220 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3356 - accuracy: 0.9136\n",
      "Epoch 92: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 113ms/step - loss: 0.3356 - accuracy: 0.9136 - val_loss: 0.3387 - val_accuracy: 0.9131 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3528 - accuracy: 0.9040\n",
      "Epoch 93: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 107ms/step - loss: 0.3528 - accuracy: 0.9040 - val_loss: 0.3178 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.9155\n",
      "Epoch 94: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3302 - accuracy: 0.9155 - val_loss: 0.3185 - val_accuracy: 0.8996 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3173 - accuracy: 0.9151\n",
      "Epoch 95: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3173 - accuracy: 0.9151 - val_loss: 0.3175 - val_accuracy: 0.9131 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.9136\n",
      "Epoch 96: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3206 - accuracy: 0.9136 - val_loss: 0.3167 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.9102\n",
      "Epoch 97: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3284 - accuracy: 0.9102 - val_loss: 0.3425 - val_accuracy: 0.8938 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.9102\n",
      "Epoch 98: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3405 - accuracy: 0.9102 - val_loss: 0.3262 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.9112\n",
      "Epoch 99: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3250 - accuracy: 0.9112 - val_loss: 0.3182 - val_accuracy: 0.9151 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.9107\n",
      "Epoch 100: val_accuracy did not improve from 0.91699\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 0.3360 - accuracy: 0.9107 - val_loss: 0.3183 - val_accuracy: 0.9151 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc955d72980>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 51ms/step - loss: 0.2920 - accuracy: 0.9270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.292035311460495, 0.9270270466804504]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_model = load_model('/tmp/30_LSTM_checkpoint.h5')\n",
    "cp_model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 2s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = CLASSES[np.argmax(y_pred, axis = -1)]\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(X_test)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    data = X_test[idx]\n",
    "    ax = fig.add_subplot(1, n_to_show, i+1)\n",
    "    ax.plot(data)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.2, 'pred = ' + str(preds_single[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n",
    "    ax.text(0.5, -0.4, 'act = ' + str(actual_single[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9810    0.9366    0.9583       662\n",
      "           1     0.9150    0.9091    0.9121       308\n",
      "           2     0.7500    0.9214    0.8269       140\n",
      "\n",
      "    accuracy                         0.9270      1110\n",
      "   macro avg     0.8820    0.9224    0.8991      1110\n",
      "weighted avg     0.9336    0.9270    0.9289      1110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_test_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        Benign  Sysrv  Xmrig\n",
      "Benign     620     22     20\n",
      "Sysrv        5    280     23\n",
      "Xmrig        7      4    129\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "class_labels = ['Benign', 'Sysrv', 'Xmrig']\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
