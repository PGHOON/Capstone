{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:41:21.955965: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 22:41:22.007448: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 22:41:22.007490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 22:41:22.007521: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 22:41:22.016518: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, Activation, Add, Flatten\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Calls List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "syscalls = [\n",
    "\"sys_enter_llistxattr\",\n",
    "\"sys_enter_setgroups\",\n",
    "\"sys_enter_lremovexattr\",\n",
    "\"sys_enter_sethostname\",\n",
    "\"sys_enter_accept\",\n",
    "\"sys_enter_lseek\",\n",
    "\"sys_enter_setitimer\",\n",
    "\"sys_enter_accept4\",\n",
    "\"sys_enter_lsetxattr\",\n",
    "\"sys_enter_setns\",\n",
    "\"sys_enter_acct\",\n",
    "\"sys_enter_madvise\",\n",
    "\"sys_enter_setpgid\",\n",
    "\"sys_enter_add_key\",\n",
    "\"sys_enter_mbind\",\n",
    "\"sys_enter_setpriority\",\n",
    "\"sys_enter_adjtimex\",\n",
    "\"sys_enter_membarrier\",\n",
    "\"sys_enter_setregid\",\n",
    "\"sys_enter_personality\",\n",
    "\"sys_enter_memfd_create\",\n",
    "\"sys_enter_setresgid\",\n",
    "\"sys_enter_bind\",\n",
    "\"sys_enter_memfd_secret\",\n",
    "\"sys_enter_setresuid\",\n",
    "\"sys_enter_bpf\",\n",
    "\"sys_enter_migrate_pages\",\n",
    "\"sys_enter_setreuid\",\n",
    "\"sys_enter_brk\",\n",
    "\"sys_enter_mincore\",\n",
    "\"sys_enter_setrlimit\",\n",
    "\"sys_enter_capget\",\n",
    "\"sys_enter_mkdirat\",\n",
    "\"sys_enter_setsid\",\n",
    "\"sys_enter_capset\",\n",
    "\"sys_enter_mknodat\",\n",
    "\"sys_enter_setsockopt\",\n",
    "\"sys_enter_chdir\",\n",
    "\"sys_enter_mlock\",\n",
    "\"sys_enter_settimeofday\",\n",
    "\"sys_enter_chroot\",\n",
    "\"sys_enter_mlock2\",\n",
    "\"sys_enter_setuid\",\n",
    "\"sys_enter_clock_adjtime\",\n",
    "\"sys_enter_mlockall\",\n",
    "\"sys_enter_setxattr\",\n",
    "\"sys_enter_clock_getres\",\n",
    "\"sys_enter_mmap\",\n",
    "\"sys_enter_shmat\",\n",
    "\"sys_enter_clock_gettime\",\n",
    "\"sys_enter_mount\",\n",
    "\"sys_enter_shmctl\",\n",
    "\"sys_enter_clock_nanosleep\",\n",
    "\"sys_enter_mount_setattr\",\n",
    "\"sys_enter_shmdt\",\n",
    "\"sys_enter_clock_settime\",\n",
    "\"sys_enter_move_mount\",\n",
    "\"sys_enter_shmget\",\n",
    "\"sys_enter_clone\",\n",
    "\"sys_enter_move_pages\",\n",
    "\"sys_enter_shutdown\",\n",
    "\"sys_enter_clone3\",\n",
    "\"sys_enter_mprotect\",\n",
    "\"sys_enter_sigaltstack\",\n",
    "\"sys_enter_close\",\n",
    "\"sys_enter_mq_getsetattr\",\n",
    "\"sys_enter_signalfd4\",\n",
    "\"sys_enter_close_range\",\n",
    "\"sys_enter_mq_notify\",\n",
    "\"sys_enter_socket\",\n",
    "\"sys_enter_connect\",\n",
    "\"sys_enter_mq_open\",\n",
    "\"sys_enter_socketpair\",\n",
    "\"sys_enter_copy_file_range\",\n",
    "\"sys_enter_mq_timedreceive\",\n",
    "\"sys_enter_splice\",\n",
    "\"sys_enter_delete_module\",\n",
    "\"sys_enter_mq_timedsend\",\n",
    "\"sys_enter_statfs\",\n",
    "\"sys_enter_dup\",\n",
    "\"sys_enter_mq_unlink\",\n",
    "\"sys_enter_statx\",\n",
    "\"sys_enter_dup3\",\n",
    "\"sys_enter_mremap\",\n",
    "\"sys_enter_swapoff\",\n",
    "\"sys_enter_epoll_create1\",\n",
    "\"sys_enter_msgctl\",\n",
    "\"sys_enter_swapon\",\n",
    "\"sys_enter_epoll_ctl\",\n",
    "\"sys_enter_msgget\",\n",
    "\"sys_enter_symlinkat\",\n",
    "\"sys_enter_epoll_pwait\",\n",
    "\"sys_enter_msgrcv\",\n",
    "\"sys_enter_sync\",\n",
    "\"sys_enter_epoll_pwait2\",\n",
    "\"sys_enter_msgsnd\",\n",
    "\"sys_enter_sync_file_range\",\n",
    "\"sys_enter_eventfd2\",\n",
    "\"sys_enter_msync\",\n",
    "\"sys_enter_syncfs\",\n",
    "\"sys_enter_execve\",\n",
    "\"sys_enter_munlock\",\n",
    "\"sys_enter_sysinfo\",\n",
    "\"sys_enter_execveat\",\n",
    "\"sys_enter_munlockall\",\n",
    "\"sys_enter_syslog\",\n",
    "\"sys_enter_exit\",\n",
    "\"sys_enter_munmap\",\n",
    "\"sys_enter_tee\",\n",
    "\"sys_enter_exit_group\",\n",
    "\"sys_enter_name_to_handle_at\",\n",
    "\"sys_enter_tgkill\",\n",
    "\"sys_enter_faccessat\",\n",
    "\"sys_enter_nanosleep\",\n",
    "\"sys_enter_timer_create\",\n",
    "\"sys_enter_faccessat2\",\n",
    "\"sys_enter_newfstat\",\n",
    "\"sys_enter_timer_delete\",\n",
    "\"sys_enter_fadvise64\",\n",
    "\"sys_enter_newfstatat\",\n",
    "\"sys_enter_timer_getoverrun\",\n",
    "\"sys_enter_fallocate\",\n",
    "\"sys_enter_newuname\",\n",
    "\"sys_enter_timer_gettime\",\n",
    "\"sys_enter_fanotify_init\",\n",
    "\"sys_enter_open_by_handle_at\",\n",
    "\"sys_enter_timer_settime\",\n",
    "\"sys_enter_fanotify_mark\",\n",
    "\"sys_enter_open_tree\",\n",
    "\"sys_enter_timerfd_create\",\n",
    "\"sys_enter_fchdir\",\n",
    "\"sys_enter_openat\",\n",
    "\"sys_enter_timerfd_gettime\",\n",
    "\"sys_enter_fchmod\",\n",
    "\"sys_enter_openat2\",\n",
    "\"sys_enter_timerfd_settime\",\n",
    "\"sys_enter_fchmodat\",\n",
    "\"sys_enter_perf_event_open\",\n",
    "\"sys_enter_times\",\n",
    "\"sys_enter_fchown\",\n",
    "\"sys_enter_pidfd_getfd\",\n",
    "\"sys_enter_tkill\",\n",
    "\"sys_enter_fchownat\",\n",
    "\"sys_enter_pidfd_open\",\n",
    "\"sys_enter_truncate\",\n",
    "\"sys_enter_fcntl\",\n",
    "\"sys_enter_pidfd_send_signal\",\n",
    "\"sys_enter_umask\",\n",
    "\"sys_enter_fdatasync\",\n",
    "\"sys_enter_pipe2\",\n",
    "\"sys_enter_umount\",\n",
    "\"sys_enter_fgetxattr\",\n",
    "\"sys_enter_pivot_root\",\n",
    "\"sys_enter_unlinkat\",\n",
    "\"sys_enter_finit_module\",\n",
    "\"sys_enter_ppoll\",\n",
    "\"sys_enter_unshare\",\n",
    "\"sys_enter_flistxattr\",\n",
    "\"sys_enter_prctl\",\n",
    "\"sys_enter_userfaultfd\",\n",
    "\"sys_enter_flock\",\n",
    "\"sys_enter_pread64\",\n",
    "\"sys_enter_utimensat\",\n",
    "\"sys_enter_fremovexattr\",\n",
    "\"sys_enter_preadv\",\n",
    "\"sys_enter_vhangup\",\n",
    "\"sys_enter_fsconfig\",\n",
    "\"sys_enter_preadv2\",\n",
    "\"sys_enter_vmsplice\",\n",
    "\"sys_enter_fsetxattr\",\n",
    "\"sys_enter_prlimit64\",\n",
    "\"sys_enter_wait4\",\n",
    "\"sys_enter_fsmount\",\n",
    "\"sys_enter_process_madvise\",\n",
    "\"sys_enter_waitid\",\n",
    "\"sys_enter_fsopen\",\n",
    "\"sys_enter_process_mrelease\",\n",
    "\"sys_enter_write\",\n",
    "\"sys_enter_fspick\",\n",
    "\"sys_enter_process_vm_readv\",\n",
    "\"sys_enter_writev\",\n",
    "\"sys_enter_fstatfs\",\n",
    "\"sys_enter_process_vm_writev\",\n",
    "\"sys_enter_fsync\",\n",
    "\"sys_enter_pselect6\",\n",
    "\"sys_enter_ftruncate\",\n",
    "\"sys_enter_ptrace\",\n",
    "\"sys_enter_futex\",\n",
    "\"sys_enter_pwrite64\",\n",
    "\"sys_enter_get_mempolicy\",\n",
    "\"sys_enter_pwritev\",\n",
    "\"sys_enter_get_robust_list\",\n",
    "\"sys_enter_pwritev2\",\n",
    "\"sys_enter_getcpu\",\n",
    "\"sys_enter_quotactl\",\n",
    "\"sys_enter_getcwd\",\n",
    "\"sys_enter_quotactl_fd\",\n",
    "\"sys_enter_getdents64\",\n",
    "\"sys_enter_read\",\n",
    "\"sys_enter_getegid\",\n",
    "\"sys_enter_readahead\",\n",
    "\"sys_enter_geteuid\",\n",
    "\"sys_enter_readlinkat\",\n",
    "\"sys_enter_getgid\",\n",
    "\"sys_enter_readv\",\n",
    "\"sys_enter_getgroups\",\n",
    "\"sys_enter_reboot\",\n",
    "\"sys_enter_getitimer\",\n",
    "\"sys_enter_recvfrom\",\n",
    "\"sys_enter_getpeername\",\n",
    "\"sys_enter_recvmmsg\",\n",
    "\"sys_enter_getpgid\",\n",
    "\"sys_enter_recvmsg\",\n",
    "\"sys_enter_getpid\",\n",
    "\"sys_enter_remap_file_pages\",\n",
    "\"sys_enter_getppid\",\n",
    "\"sys_enter_removexattr\",\n",
    "\"sys_enter_getpriority\",\n",
    "\"sys_enter_renameat\",\n",
    "\"sys_enter_getrandom\",\n",
    "\"sys_enter_renameat2\",\n",
    "\"sys_enter_getresgid\",\n",
    "\"sys_enter_request_key\",\n",
    "\"sys_enter_getresuid\",\n",
    "\"sys_enter_restart_syscall\",\n",
    "\"sys_enter_getrlimit\",\n",
    "\"sys_enter_rseq\",\n",
    "\"sys_enter_getrusage\",\n",
    "\"sys_enter_rt_sigaction\",\n",
    "\"sys_enter_getsid\",\n",
    "\"sys_enter_rt_sigpending\",\n",
    "\"sys_enter_getsockname\",\n",
    "\"sys_enter_rt_sigprocmask\",\n",
    "\"sys_enter_getsockopt\",\n",
    "\"sys_enter_rt_sigqueueinfo\",\n",
    "\"sys_enter_gettid\",\n",
    "\"sys_enter_rt_sigreturn\",\n",
    "\"sys_enter_gettimeofday\",\n",
    "\"sys_enter_rt_sigsuspend\",\n",
    "\"sys_enter_getuid\",\n",
    "\"sys_enter_rt_sigtimedwait\",\n",
    "\"sys_enter_getxattr\",\n",
    "\"sys_enter_rt_tgsigqueueinfo\",\n",
    "\"sys_enter_init_module\",\n",
    "\"sys_enter_sched_get_priority_max\",\n",
    "\"sys_enter_inotify_add_watch\",\n",
    "\"sys_enter_sched_get_priority_min\",\n",
    "\"sys_enter_inotify_init1\",\n",
    "\"sys_enter_sched_getaffinity\",\n",
    "\"sys_enter_inotify_rm_watch\",\n",
    "\"sys_enter_sched_getattr\",\n",
    "\"sys_enter_io_cancel\",\n",
    "\"sys_enter_sched_getparam\",\n",
    "\"sys_enter_io_destroy\",\n",
    "\"sys_enter_sched_getscheduler\",\n",
    "\"sys_enter_io_getevents\",\n",
    "\"sys_enter_sched_rr_get_interval\",\n",
    "\"sys_enter_io_pgetevents\",\n",
    "\"sys_enter_sched_setaffinity\",\n",
    "\"sys_enter_io_setup\",\n",
    "\"sys_enter_sched_setattr\",\n",
    "\"sys_enter_io_submit\",\n",
    "\"sys_enter_sched_setparam\",\n",
    "\"sys_enter_io_uring_enter\",\n",
    "\"sys_enter_sched_setscheduler\",\n",
    "\"sys_enter_io_uring_register\",\n",
    "\"sys_enter_sched_yield\",\n",
    "\"sys_enter_io_uring_setup\",\n",
    "\"sys_enter_seccomp\",\n",
    "\"sys_enter_ioctl\",\n",
    "\"sys_enter_semctl\",\n",
    "\"sys_enter_ioprio_get\",\n",
    "\"sys_enter_semget\",\n",
    "\"sys_enter_ioprio_set\",\n",
    "\"sys_enter_semop\",\n",
    "\"sys_enter_kcmp\",\n",
    "\"sys_enter_semtimedop\",\n",
    "\"sys_enter_kexec_file_load\",\n",
    "\"sys_enter_sendfile64\",\n",
    "\"sys_enter_kexec_load\",\n",
    "\"sys_enter_sendmmsg\",\n",
    "\"sys_enter_keyctl\",\n",
    "\"sys_enter_sendmsg\",\n",
    "\"sys_enter_kill\",\n",
    "\"sys_enter_sendto\",\n",
    "\"sys_enter_landlock_add_rule\",\n",
    "\"sys_enter_set_mempolicy\",\n",
    "\"sys_enter_landlock_create_ruleset\",\n",
    "\"sys_enter_set_robust_list\",\n",
    "\"sys_enter_landlock_restrict_self\",\n",
    "\"sys_enter_set_tid_address\",\n",
    "\"sys_enter_lgetxattr\",\n",
    "\"sys_enter_setdomainname\",\n",
    "\"sys_enter_linkat\",\n",
    "\"sys_enter_setfsgid\",\n",
    "\"sys_enter_listen\",\n",
    "\"sys_enter_setfsuid\",\n",
    "\"sys_enter_listxattr\",\n",
    "\"sys_enter_setgid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV from Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "CLASSES = np.array(['benign', 'sysrv', 'xmrig'])\n",
    "DATASET_DIR = \"raw_data\"\n",
    "VECTOR_LENGTH = 32 * 32\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(syscalls)\n",
    "\n",
    "def csvToVector(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    \n",
    "    data_encoded = label_encoder.fit_transform(data['SYSTEM_CALL'])\n",
    "    vector = np.zeros(VECTOR_LENGTH, dtype=np.uint8)\n",
    "    syscall_nums = min(len(data_encoded), VECTOR_LENGTH)\n",
    "    vector[:syscall_nums] = data_encoded[:syscall_nums]\n",
    "\n",
    "    return vector\n",
    "\n",
    "def process_file(args):\n",
    "    file_path, class_idx = args\n",
    "    vector = csvToVector(file_path)\n",
    "    return vector, class_idx\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    classes = [\"0/90sec_0\", \"1/90sec_1\", \"2/90sec_2\"]\n",
    "\n",
    "    file_paths = []\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(dataset_dir, class_name)\n",
    "        for file_name in os.listdir(class_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                file_paths.append((file_path, class_idx))\n",
    "\n",
    "    with Pool() as pool:\n",
    "        results = pool.map(process_file, file_paths)\n",
    "\n",
    "    x, y = zip(*results)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Split and Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 299.0\n",
    "X_val = X_val / 299.0\n",
    "X_test = X_test / 299.0\n",
    "\n",
    "y_train = to_categorical(y_train, 3)\n",
    "y_val = to_categorical(y_val, 3)\n",
    "y_test = to_categorical(y_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(893, 1024)\n",
      "(479, 1024)\n",
      "(893, 3)\n",
      "(479, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:55:58.637785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-29 17:55:58.638316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-29 17:55:58.638800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(VECTOR_LENGTH, 1))\n",
    "\n",
    "x = Conv1D(filters=32, kernel_size=3, padding='same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=64, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1024, 1)]         0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1024, 32)          128       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024, 32)          128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1024, 32)          0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 512, 32)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 512, 64)           6208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512, 64)           256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 512, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 256, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 256, 128)          24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 256, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 256, 128)          0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 128, 128)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4194560   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4228291 (16.13 MB)\n",
      "Trainable params: 4227331 (16.13 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/tmp/90_CNN1d_checkpoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 17:56:01.603357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-07-29 17:56:03.011596: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efd35dbf2d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-29 17:56:03.011632: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:56:03.011638: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:56:03.011643: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-29 17:56:03.017447: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-29 17:56:03.110964: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/28 [===========================>..] - ETA: 0s - loss: 0.6270 - accuracy: 0.8322\n",
      "Epoch 1: accuracy improved from -inf to 0.83427, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 6s 26ms/step - loss: 0.6144 - accuracy: 0.8343 - val_loss: 1.0623 - val_accuracy: 0.3705 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "10/28 [=========>....................] - ETA: 0s - loss: 0.3363 - accuracy: 0.8938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - ETA: 0s - loss: 0.3446 - accuracy: 0.8869\n",
      "Epoch 2: accuracy improved from 0.83427 to 0.88690, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 0.3446 - accuracy: 0.8869 - val_loss: 0.9849 - val_accuracy: 0.6205 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.9250\n",
      "Epoch 3: accuracy improved from 0.88690 to 0.92497, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 0.2397 - accuracy: 0.9250 - val_loss: 1.0211 - val_accuracy: 0.6205 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 0.9239\n",
      "Epoch 4: accuracy did not improve from 0.92497\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.2267 - accuracy: 0.9239 - val_loss: 1.0447 - val_accuracy: 0.1652 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9485\n",
      "Epoch 5: accuracy improved from 0.92497 to 0.94849, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.1711 - accuracy: 0.9485 - val_loss: 1.1559 - val_accuracy: 0.1429 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9406\n",
      "Epoch 6: accuracy did not improve from 0.94849\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.2147 - accuracy: 0.9406 - val_loss: 1.3572 - val_accuracy: 0.1429 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9597\n",
      "Epoch 7: accuracy improved from 0.94849 to 0.95969, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.1482 - accuracy: 0.9597 - val_loss: 1.6537 - val_accuracy: 0.1562 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9518\n",
      "Epoch 8: accuracy did not improve from 0.95969\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1591 - accuracy: 0.9518 - val_loss: 1.7353 - val_accuracy: 0.1473 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1836 - accuracy: 0.9373\n",
      "Epoch 9: accuracy did not improve from 0.95969\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1836 - accuracy: 0.9373 - val_loss: 1.9554 - val_accuracy: 0.1562 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9496\n",
      "Epoch 10: accuracy did not improve from 0.95969\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1650 - accuracy: 0.9496 - val_loss: 2.2038 - val_accuracy: 0.1562 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1495 - accuracy: 0.9597\n",
      "Epoch 11: accuracy did not improve from 0.95969\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1495 - accuracy: 0.9597 - val_loss: 2.2007 - val_accuracy: 0.1473 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9608\n",
      "Epoch 12: accuracy improved from 0.95969 to 0.96081, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.1446 - accuracy: 0.9608 - val_loss: 2.5978 - val_accuracy: 0.1473 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.1335 - accuracy: 0.9618\n",
      "Epoch 13: accuracy improved from 0.96081 to 0.96305, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.1302 - accuracy: 0.9630 - val_loss: 2.4889 - val_accuracy: 0.1875 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9630\n",
      "Epoch 14: accuracy did not improve from 0.96305\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1255 - accuracy: 0.9630 - val_loss: 2.7920 - val_accuracy: 0.2768 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9541\n",
      "Epoch 15: accuracy did not improve from 0.96305\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1518 - accuracy: 0.9541 - val_loss: 2.9893 - val_accuracy: 0.2991 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9619\n",
      "Epoch 16: accuracy did not improve from 0.96305\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1363 - accuracy: 0.9619 - val_loss: 3.0936 - val_accuracy: 0.3170 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9664\n",
      "Epoch 17: accuracy improved from 0.96305 to 0.96641, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.1230 - accuracy: 0.9664 - val_loss: 2.9240 - val_accuracy: 0.3348 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9642\n",
      "Epoch 18: accuracy did not improve from 0.96641\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1158 - accuracy: 0.9642 - val_loss: 2.6743 - val_accuracy: 0.3527 - lr: 1.2500e-04\n",
      "Epoch 19/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9709\n",
      "Epoch 19: accuracy improved from 0.96641 to 0.97088, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.1315 - accuracy: 0.9709 - val_loss: 2.3888 - val_accuracy: 0.3616 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9686\n",
      "Epoch 20: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1247 - accuracy: 0.9686 - val_loss: 1.7904 - val_accuracy: 0.3973 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9597\n",
      "Epoch 21: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1484 - accuracy: 0.9597 - val_loss: 1.5489 - val_accuracy: 0.2902 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9608\n",
      "Epoch 22: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1281 - accuracy: 0.9608 - val_loss: 1.5377 - val_accuracy: 0.4196 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1270 - accuracy: 0.9675\n",
      "Epoch 23: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1270 - accuracy: 0.9675 - val_loss: 1.2214 - val_accuracy: 0.4286 - lr: 6.2500e-05\n",
      "Epoch 24/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9675\n",
      "Epoch 24: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1232 - accuracy: 0.9675 - val_loss: 0.7197 - val_accuracy: 0.5580 - lr: 6.2500e-05\n",
      "Epoch 25/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9653\n",
      "Epoch 25: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1289 - accuracy: 0.9653 - val_loss: 0.2582 - val_accuracy: 0.9509 - lr: 6.2500e-05\n",
      "Epoch 26/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9686\n",
      "Epoch 26: accuracy did not improve from 0.97088\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1119 - accuracy: 0.9686 - val_loss: 0.1758 - val_accuracy: 0.9643 - lr: 6.2500e-05\n",
      "Epoch 27/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9776\n",
      "Epoch 27: accuracy improved from 0.97088 to 0.97760, saving model to /tmp/90_CNN1d_checkpoint.h5\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.1263 - accuracy: 0.9776 - val_loss: 0.2115 - val_accuracy: 0.9643 - lr: 6.2500e-05\n",
      "Epoch 28/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9720\n",
      "Epoch 28: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1077 - accuracy: 0.9720 - val_loss: 0.1228 - val_accuracy: 0.9643 - lr: 6.2500e-05\n",
      "Epoch 29/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.1021 - accuracy: 0.9711\n",
      "Epoch 29: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1000 - accuracy: 0.9720 - val_loss: 0.1232 - val_accuracy: 0.9688 - lr: 6.2500e-05\n",
      "Epoch 30/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9742\n",
      "Epoch 30: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1021 - accuracy: 0.9742 - val_loss: 0.1264 - val_accuracy: 0.9688 - lr: 6.2500e-05\n",
      "Epoch 31/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9709\n",
      "Epoch 31: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1156 - accuracy: 0.9709 - val_loss: 0.1272 - val_accuracy: 0.9688 - lr: 6.2500e-05\n",
      "Epoch 32/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9720\n",
      "Epoch 32: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1209 - accuracy: 0.9720 - val_loss: 0.1518 - val_accuracy: 0.9688 - lr: 6.2500e-05\n",
      "Epoch 33/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9731\n",
      "Epoch 33: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1066 - accuracy: 0.9731 - val_loss: 0.1423 - val_accuracy: 0.9688 - lr: 6.2500e-05\n",
      "Epoch 34/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9698\n",
      "Epoch 34: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1005 - accuracy: 0.9698 - val_loss: 0.1204 - val_accuracy: 0.9643 - lr: 3.1250e-05\n",
      "Epoch 35/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9563\n",
      "Epoch 35: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1295 - accuracy: 0.9563 - val_loss: 0.1144 - val_accuracy: 0.9643 - lr: 3.1250e-05\n",
      "Epoch 36/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9698\n",
      "Epoch 36: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1209 - accuracy: 0.9698 - val_loss: 0.1376 - val_accuracy: 0.9643 - lr: 3.1250e-05\n",
      "Epoch 37/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9709\n",
      "Epoch 37: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1179 - accuracy: 0.9709 - val_loss: 0.1388 - val_accuracy: 0.9643 - lr: 3.1250e-05\n",
      "Epoch 38/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9675\n",
      "Epoch 38: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1207 - accuracy: 0.9675 - val_loss: 0.2255 - val_accuracy: 0.9420 - lr: 3.1250e-05\n",
      "Epoch 39/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9709\n",
      "Epoch 39: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1021 - accuracy: 0.9709 - val_loss: 0.2052 - val_accuracy: 0.9420 - lr: 3.1250e-05\n",
      "Epoch 40/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9653\n",
      "Epoch 40: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1021 - accuracy: 0.9653 - val_loss: 0.1437 - val_accuracy: 0.9688 - lr: 3.1250e-05\n",
      "Epoch 41/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9709\n",
      "Epoch 41: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1161 - accuracy: 0.9709 - val_loss: 0.1319 - val_accuracy: 0.9643 - lr: 1.5625e-05\n",
      "Epoch 42/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9698\n",
      "Epoch 42: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1107 - accuracy: 0.9698 - val_loss: 0.1367 - val_accuracy: 0.9643 - lr: 1.5625e-05\n",
      "Epoch 43/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9754\n",
      "Epoch 43: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1085 - accuracy: 0.9754 - val_loss: 0.1294 - val_accuracy: 0.9643 - lr: 1.5625e-05\n",
      "Epoch 44/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9720\n",
      "Epoch 44: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1102 - accuracy: 0.9720 - val_loss: 0.1256 - val_accuracy: 0.9688 - lr: 1.5625e-05\n",
      "Epoch 45/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9731\n",
      "Epoch 45: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1062 - accuracy: 0.9731 - val_loss: 0.1203 - val_accuracy: 0.9643 - lr: 1.5625e-05\n",
      "Epoch 46/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9675\n",
      "Epoch 46: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1189 - accuracy: 0.9675 - val_loss: 0.1141 - val_accuracy: 0.9688 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9742\n",
      "Epoch 47: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1021 - accuracy: 0.9742 - val_loss: 0.1122 - val_accuracy: 0.9688 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9686\n",
      "Epoch 48: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1065 - accuracy: 0.9686 - val_loss: 0.1137 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1146 - accuracy: 0.9731\n",
      "Epoch 49: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1146 - accuracy: 0.9731 - val_loss: 0.1169 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.1105 - accuracy: 0.9711\n",
      "Epoch 50: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1171 - accuracy: 0.9698 - val_loss: 0.1162 - val_accuracy: 0.9688 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9698\n",
      "Epoch 51: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1032 - accuracy: 0.9698 - val_loss: 0.1149 - val_accuracy: 0.9688 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9731\n",
      "Epoch 52: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1104 - accuracy: 0.9731 - val_loss: 0.1151 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9742\n",
      "Epoch 53: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0987 - accuracy: 0.9742 - val_loss: 0.1137 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9709\n",
      "Epoch 54: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1163 - accuracy: 0.9709 - val_loss: 0.1140 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9720\n",
      "Epoch 55: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1047 - accuracy: 0.9720 - val_loss: 0.1155 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9709\n",
      "Epoch 56: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1104 - accuracy: 0.9709 - val_loss: 0.1167 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1198 - accuracy: 0.9731\n",
      "Epoch 57: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9731 - val_loss: 0.1161 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9731\n",
      "Epoch 58: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0948 - accuracy: 0.9731 - val_loss: 0.1141 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9686\n",
      "Epoch 59: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1093 - accuracy: 0.9686 - val_loss: 0.1155 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9720\n",
      "Epoch 60: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0998 - accuracy: 0.9720 - val_loss: 0.1173 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1011 - accuracy: 0.9731\n",
      "Epoch 61: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1011 - accuracy: 0.9731 - val_loss: 0.1162 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9709\n",
      "Epoch 62: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1056 - accuracy: 0.9709 - val_loss: 0.1156 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9709\n",
      "Epoch 63: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1073 - accuracy: 0.9709 - val_loss: 0.1151 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9754\n",
      "Epoch 64: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0868 - accuracy: 0.9754 - val_loss: 0.1163 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1003 - accuracy: 0.9742\n",
      "Epoch 65: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1003 - accuracy: 0.9742 - val_loss: 0.1159 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9664\n",
      "Epoch 66: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1029 - accuracy: 0.9664 - val_loss: 0.1154 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9709\n",
      "Epoch 67: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1140 - accuracy: 0.9709 - val_loss: 0.1163 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9698\n",
      "Epoch 68: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1077 - accuracy: 0.9698 - val_loss: 0.1146 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9754\n",
      "Epoch 69: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0917 - accuracy: 0.9754 - val_loss: 0.1163 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1031 - accuracy: 0.9731\n",
      "Epoch 70: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1031 - accuracy: 0.9731 - val_loss: 0.1166 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9720\n",
      "Epoch 71: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1051 - accuracy: 0.9720 - val_loss: 0.1152 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9731\n",
      "Epoch 72: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0951 - accuracy: 0.9731 - val_loss: 0.1164 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9709\n",
      "Epoch 73: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1126 - accuracy: 0.9709 - val_loss: 0.1155 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9754\n",
      "Epoch 74: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0981 - accuracy: 0.9754 - val_loss: 0.1151 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9731\n",
      "Epoch 75: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0942 - accuracy: 0.9731 - val_loss: 0.1155 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9698\n",
      "Epoch 76: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1175 - accuracy: 0.9698 - val_loss: 0.1158 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9731\n",
      "Epoch 77: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0970 - accuracy: 0.9731 - val_loss: 0.1192 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9754\n",
      "Epoch 78: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0944 - accuracy: 0.9754 - val_loss: 0.1191 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9754\n",
      "Epoch 79: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0902 - accuracy: 0.9754 - val_loss: 0.1175 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9686\n",
      "Epoch 80: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1026 - accuracy: 0.9686 - val_loss: 0.1164 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9720\n",
      "Epoch 81: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1087 - accuracy: 0.9720 - val_loss: 0.1175 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9709\n",
      "Epoch 82: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1115 - accuracy: 0.9709 - val_loss: 0.1178 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9742\n",
      "Epoch 83: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1047 - accuracy: 0.9742 - val_loss: 0.1168 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9709\n",
      "Epoch 84: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1085 - accuracy: 0.9709 - val_loss: 0.1177 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9754\n",
      "Epoch 85: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0988 - accuracy: 0.9754 - val_loss: 0.1163 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9709\n",
      "Epoch 86: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1148 - accuracy: 0.9709 - val_loss: 0.1170 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9742\n",
      "Epoch 87: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0968 - accuracy: 0.9742 - val_loss: 0.1166 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9720\n",
      "Epoch 88: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9720 - val_loss: 0.1162 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9709\n",
      "Epoch 89: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0935 - accuracy: 0.9709 - val_loss: 0.1168 - val_accuracy: 0.9598 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9765\n",
      "Epoch 90: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0969 - accuracy: 0.9765 - val_loss: 0.1150 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9765\n",
      "Epoch 91: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0977 - accuracy: 0.9765 - val_loss: 0.1160 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9754\n",
      "Epoch 92: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0983 - accuracy: 0.9754 - val_loss: 0.1153 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9731\n",
      "Epoch 93: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1132 - accuracy: 0.9731 - val_loss: 0.1144 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9720\n",
      "Epoch 94: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1058 - accuracy: 0.9720 - val_loss: 0.1140 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9675\n",
      "Epoch 95: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1030 - accuracy: 0.9675 - val_loss: 0.1136 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9742\n",
      "Epoch 96: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.0993 - accuracy: 0.9742 - val_loss: 0.1157 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1215 - accuracy: 0.9653\n",
      "Epoch 97: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1215 - accuracy: 0.9653 - val_loss: 0.1147 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9742\n",
      "Epoch 98: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1066 - accuracy: 0.9742 - val_loss: 0.1142 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9630\n",
      "Epoch 99: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1172 - accuracy: 0.9630 - val_loss: 0.1144 - val_accuracy: 0.9643 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9720\n",
      "Epoch 100: accuracy did not improve from 0.97760\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.1036 - accuracy: 0.9720 - val_loss: 0.1162 - val_accuracy: 0.9643 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f008926b820>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 22:41:39.543787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-31 22:41:39.544345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-31 22:41:39.544833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2024-07-31 22:41:41.014154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 699ms/step - loss: 0.2909 - accuracy: 0.9311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29086992144584656, 0.931106448173523]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_model = load_model('/tmp/90_CNN1d_checkpoint.h5')\n",
    "cp_model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = CLASSES[np.argmax(y_pred, axis = -1)]\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(X_test)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    data = X_test[idx]\n",
    "    ax = fig.add_subplot(1, n_to_show, i+1)\n",
    "    ax.plot(data)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.2, 'pred = ' + str(preds_single[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n",
    "    ax.text(0.5, -0.4, 'act = ' + str(actual_single[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9429    0.9706    0.9565       272\n",
      "           1     0.9560    0.8529    0.9016       102\n",
      "           2     0.8796    0.9048    0.8920       105\n",
      "\n",
      "    accuracy                         0.9311       479\n",
      "   macro avg     0.9262    0.9094    0.9167       479\n",
      "weighted avg     0.9318    0.9311    0.9307       479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_test_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "        Benign  Sysrv  Xmrig\n",
      "Benign     264      2      6\n",
      "Sysrv        8     87      7\n",
      "Xmrig        8      2     95\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "class_labels = ['Benign', 'Sysrv', 'Xmrig']\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
