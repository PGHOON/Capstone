{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 23:15:41.897323: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 23:15:41.948910: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9360] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 23:15:41.948950: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 23:15:41.948982: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1537] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 23:15:41.957930: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Conv1D, BatchNormalization, LeakyReLU, MaxPooling1D, GlobalAveragePooling1D, Dense, Dropout, Activation, Add, Flatten\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load csv from Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "CLASSES = np.array(['Legitimate', 'Suspicious'])\n",
    "DATASET_DIR = \"./\"\n",
    "VECTOR_LENGTH = 1 * 816\n",
    "\n",
    "def csvToVector(file_path):\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    vector = data.values.flatten()\n",
    "    return vector\n",
    "\n",
    "def process_file(class_idx, file_path):\n",
    "    vector = csvToVector(file_path)\n",
    "    return (vector, class_idx)\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "    subdirs = ['benign_cms1', 'malware_cms1']\n",
    "    futures = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for class_idx, class_name in enumerate(subdirs):\n",
    "            class_dir = os.path.join(dataset_dir, class_name)\n",
    "            for file_name in os.listdir(class_dir):\n",
    "                if file_name.endswith('.csv'):\n",
    "                    file_path = os.path.join(class_dir, file_name)\n",
    "                    futures.append(executor.submit(process_file, class_idx, file_path))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            vector, class_idx = future.result()\n",
    "            X.append(vector)\n",
    "            y.append(class_idx)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4020, 816)\n",
      "(4020,)\n",
      "[[  321     0     0 ... 10041     0   242]\n",
      " [  284     0     0 ... 12129     0    70]\n",
      " [  192     0     0 ...  9783     0    27]\n",
      " ...\n",
      " [  189     0     0 ... 48077     0    33]\n",
      " [  186     0     0 ...  2669     0    57]\n",
      " [  148     0     0 ...  2669     0    54]]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation, Test Split and Nomalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train / 299.0\n",
    "X_val = X_val / 299.0\n",
    "X_test = X_test / 299.0\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_val = to_categorical(y_val, 2)\n",
    "y_test = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2251, 816)\n",
      "(1206, 816)\n",
      "(2251, 2)\n",
      "(1206, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "#print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:12:08.310601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-30 11:12:08.311154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-30 11:12:08.311658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(VECTOR_LENGTH, 1))\n",
    "\n",
    "x = Conv1D(filters=32, kernel_size=3, padding='same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=64, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=128, kernel_size=3, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "x = Dense(NUM_CLASSES)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 816, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 816, 32)              128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 816, 32)              128       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 816, 32)              0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 408, 32)              0         ['leaky_re_lu[0][0]']         \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 408, 32)              3104      ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 408, 32)              128       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 408, 32)              0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 408, 32)              3104      ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 408, 32)              128       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 408, 32)              0         ['max_pooling1d[0][0]',       \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 408, 32)              0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 204, 32)              0         ['leaky_re_lu_2[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 204, 64)              6208      ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 204, 64)              256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 204, 64)              0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPoolin  (None, 102, 64)              0         ['leaky_re_lu_3[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 102, 64)              12352     ['max_pooling1d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 102, 64)              256       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 102, 64)              0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 102, 64)              12352     ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 102, 64)              256       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 102, 64)              0         ['max_pooling1d_2[0][0]',     \n",
      "                                                                     'batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 102, 64)              0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPoolin  (None, 51, 64)               0         ['leaky_re_lu_5[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 51, 128)              24704     ['max_pooling1d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 51, 128)              512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 51, 128)              0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 25, 128)              0         ['leaky_re_lu_6[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 25, 128)              49280     ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 25, 128)              512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 25, 128)              0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 25, 128)              49280     ['leaky_re_lu_7[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 25, 128)              512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 25, 128)              0         ['max_pooling1d_4[0][0]',     \n",
      "                                                                     'batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 25, 128)              0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 12, 128)              0         ['leaky_re_lu_8[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['max_pooling1d_5[0][0]']     \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256)                  33024     ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 256)                  1024      ['dense[0][0]']               \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 256)                  0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 256)                  0         ['leaky_re_lu_9[0][0]']       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2)                    514       ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 2)                    0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 197762 (772.51 KB)\n",
      "Trainable params: 195906 (765.26 KB)\n",
      "Non-trainable params: 1856 (7.25 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='/tmp/CMS_CNN_1D_CheckPoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:12:13.507004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-07-30 11:12:14.942106: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6e6627d650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-30 11:12:14.942141: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-30 11:12:14.942147: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-30 11:12:14.942153: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): CUDA GPU, Compute Capability 7.0\n",
      "2024-07-30 11:12:14.948058: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-30 11:12:15.041581: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - ETA: 0s - loss: 0.3576 - accuracy: 0.8570\n",
      "Epoch 1: val_accuracy improved from -inf to 0.87744, saving model to /tmp/CMS_CNN_1D_CheckPoint.h5\n",
      "71/71 [==============================] - 10s 26ms/step - loss: 0.3576 - accuracy: 0.8570 - val_loss: 0.3548 - val_accuracy: 0.8774 - lr: 0.0010\n",
      "Epoch 2/100\n",
      " 5/71 [=>............................] - ETA: 0s - loss: 0.2370 - accuracy: 0.9000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/71 [============================>.] - ETA: 0s - loss: 0.2215 - accuracy: 0.9185\n",
      "Epoch 2: val_accuracy did not improve from 0.87744\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.2241 - accuracy: 0.9169 - val_loss: 0.4576 - val_accuracy: 0.8224 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9307\n",
      "Epoch 3: val_accuracy improved from 0.87744 to 0.89343, saving model to /tmp/CMS_CNN_1D_CheckPoint.h5\n",
      "71/71 [==============================] - 1s 16ms/step - loss: 0.1903 - accuracy: 0.9307 - val_loss: 0.2545 - val_accuracy: 0.8934 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9424\n",
      "Epoch 4: val_accuracy did not improve from 0.89343\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1738 - accuracy: 0.9418 - val_loss: 0.5987 - val_accuracy: 0.8135 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9484\n",
      "Epoch 5: val_accuracy did not improve from 0.89343\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1380 - accuracy: 0.9489 - val_loss: 0.5995 - val_accuracy: 0.8206 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9565\n",
      "Epoch 6: val_accuracy did not improve from 0.89343\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1320 - accuracy: 0.9551 - val_loss: 0.4213 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9520\n",
      "Epoch 7: val_accuracy improved from 0.89343 to 0.94849, saving model to /tmp/CMS_CNN_1D_CheckPoint.h5\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.1253 - accuracy: 0.9520 - val_loss: 0.2051 - val_accuracy: 0.9485 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.1369 - accuracy: 0.9499\n",
      "Epoch 8: val_accuracy did not improve from 0.94849\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1374 - accuracy: 0.9489 - val_loss: 0.2091 - val_accuracy: 0.9378 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9605\n",
      "Epoch 9: val_accuracy improved from 0.94849 to 0.97158, saving model to /tmp/CMS_CNN_1D_CheckPoint.h5\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.1218 - accuracy: 0.9605 - val_loss: 0.0859 - val_accuracy: 0.9716 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9583\n",
      "Epoch 10: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1141 - accuracy: 0.9587 - val_loss: 0.1330 - val_accuracy: 0.9574 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9619\n",
      "Epoch 11: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1131 - accuracy: 0.9618 - val_loss: 0.1069 - val_accuracy: 0.9520 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9574\n",
      "Epoch 12: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1180 - accuracy: 0.9574 - val_loss: 0.1506 - val_accuracy: 0.9449 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.1198 - accuracy: 0.9573\n",
      "Epoch 13: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1240 - accuracy: 0.9556 - val_loss: 2.0581 - val_accuracy: 0.5684 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.1309 - accuracy: 0.9504\n",
      "Epoch 14: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1290 - accuracy: 0.9511 - val_loss: 0.4301 - val_accuracy: 0.7744 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.1117 - accuracy: 0.9606\n",
      "Epoch 15: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1124 - accuracy: 0.9600 - val_loss: 0.3641 - val_accuracy: 0.8046 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9618\n",
      "Epoch 16: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1029 - accuracy: 0.9618 - val_loss: 0.0896 - val_accuracy: 0.9609 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9590\n",
      "Epoch 17: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1229 - accuracy: 0.9578 - val_loss: 0.1030 - val_accuracy: 0.9698 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9636\n",
      "Epoch 18: val_accuracy did not improve from 0.97158\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1012 - accuracy: 0.9636 - val_loss: 0.1057 - val_accuracy: 0.9698 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9665\n",
      "Epoch 19: val_accuracy improved from 0.97158 to 0.97691, saving model to /tmp/CMS_CNN_1D_CheckPoint.h5\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.0937 - accuracy: 0.9667 - val_loss: 0.0844 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9598\n",
      "Epoch 20: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.1013 - accuracy: 0.9600 - val_loss: 0.0799 - val_accuracy: 0.9769 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9642\n",
      "Epoch 21: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0932 - accuracy: 0.9640 - val_loss: 0.2409 - val_accuracy: 0.9183 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9669\n",
      "Epoch 22: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0963 - accuracy: 0.9667 - val_loss: 0.0830 - val_accuracy: 0.9627 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9636\n",
      "Epoch 23: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0928 - accuracy: 0.9636 - val_loss: 0.1066 - val_accuracy: 0.9751 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9671\n",
      "Epoch 24: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0947 - accuracy: 0.9671 - val_loss: 0.1059 - val_accuracy: 0.9716 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0965 - accuracy: 0.9655\n",
      "Epoch 25: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0965 - accuracy: 0.9653 - val_loss: 0.1020 - val_accuracy: 0.9663 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0922 - accuracy: 0.9660\n",
      "Epoch 26: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0939 - accuracy: 0.9658 - val_loss: 0.0739 - val_accuracy: 0.9751 - lr: 2.5000e-04\n",
      "Epoch 27/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9676\n",
      "Epoch 27: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0884 - accuracy: 0.9676 - val_loss: 0.1060 - val_accuracy: 0.9591 - lr: 2.5000e-04\n",
      "Epoch 28/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0909 - accuracy: 0.9665\n",
      "Epoch 28: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0897 - accuracy: 0.9667 - val_loss: 0.1125 - val_accuracy: 0.9734 - lr: 2.5000e-04\n",
      "Epoch 29/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9683\n",
      "Epoch 29: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0848 - accuracy: 0.9680 - val_loss: 0.1527 - val_accuracy: 0.9556 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0937 - accuracy: 0.9641\n",
      "Epoch 30: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0928 - accuracy: 0.9645 - val_loss: 0.1206 - val_accuracy: 0.9716 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0783 - accuracy: 0.9692\n",
      "Epoch 31: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0844 - accuracy: 0.9662 - val_loss: 0.1806 - val_accuracy: 0.9307 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0791 - accuracy: 0.9688\n",
      "Epoch 32: val_accuracy did not improve from 0.97691\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0805 - accuracy: 0.9680 - val_loss: 0.1200 - val_accuracy: 0.9609 - lr: 1.2500e-04\n",
      "Epoch 33/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9685\n",
      "Epoch 33: val_accuracy improved from 0.97691 to 0.97869, saving model to /tmp/CMS_CNN_1D_CheckPoint.h5\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.0792 - accuracy: 0.9685 - val_loss: 0.0726 - val_accuracy: 0.9787 - lr: 1.2500e-04\n",
      "Epoch 34/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9649\n",
      "Epoch 34: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0850 - accuracy: 0.9649 - val_loss: 0.0814 - val_accuracy: 0.9751 - lr: 1.2500e-04\n",
      "Epoch 35/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9701\n",
      "Epoch 35: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0762 - accuracy: 0.9702 - val_loss: 0.0702 - val_accuracy: 0.9751 - lr: 1.2500e-04\n",
      "Epoch 36/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0744 - accuracy: 0.9715\n",
      "Epoch 36: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0762 - accuracy: 0.9711 - val_loss: 0.1946 - val_accuracy: 0.9218 - lr: 1.2500e-04\n",
      "Epoch 37/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9688\n",
      "Epoch 37: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0801 - accuracy: 0.9689 - val_loss: 0.0681 - val_accuracy: 0.9787 - lr: 1.2500e-04\n",
      "Epoch 38/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9706\n",
      "Epoch 38: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0784 - accuracy: 0.9707 - val_loss: 0.0898 - val_accuracy: 0.9698 - lr: 1.2500e-04\n",
      "Epoch 39/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9728\n",
      "Epoch 39: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0751 - accuracy: 0.9729 - val_loss: 0.2502 - val_accuracy: 0.8899 - lr: 1.2500e-04\n",
      "Epoch 40/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9665\n",
      "Epoch 40: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0788 - accuracy: 0.9671 - val_loss: 0.0759 - val_accuracy: 0.9769 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0770 - accuracy: 0.9688\n",
      "Epoch 41: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0766 - accuracy: 0.9689 - val_loss: 0.0785 - val_accuracy: 0.9751 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9706\n",
      "Epoch 42: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0754 - accuracy: 0.9707 - val_loss: 0.2728 - val_accuracy: 0.8544 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9716\n",
      "Epoch 43: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0747 - accuracy: 0.9716 - val_loss: 0.0827 - val_accuracy: 0.9734 - lr: 6.2500e-05\n",
      "Epoch 44/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9754\n",
      "Epoch 44: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0669 - accuracy: 0.9751 - val_loss: 0.1038 - val_accuracy: 0.9663 - lr: 6.2500e-05\n",
      "Epoch 45/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9665\n",
      "Epoch 45: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0777 - accuracy: 0.9667 - val_loss: 0.0739 - val_accuracy: 0.9769 - lr: 6.2500e-05\n",
      "Epoch 46/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9751\n",
      "Epoch 46: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0759 - accuracy: 0.9747 - val_loss: 0.0742 - val_accuracy: 0.9769 - lr: 6.2500e-05\n",
      "Epoch 47/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0670 - accuracy: 0.9733\n",
      "Epoch 47: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0691 - accuracy: 0.9729 - val_loss: 0.1081 - val_accuracy: 0.9645 - lr: 6.2500e-05\n",
      "Epoch 48/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9720\n",
      "Epoch 48: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0693 - accuracy: 0.9720 - val_loss: 0.0743 - val_accuracy: 0.9769 - lr: 3.1250e-05\n",
      "Epoch 49/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9746\n",
      "Epoch 49: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0664 - accuracy: 0.9747 - val_loss: 0.0734 - val_accuracy: 0.9769 - lr: 3.1250e-05\n",
      "Epoch 50/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0695 - accuracy: 0.9715\n",
      "Epoch 50: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0682 - accuracy: 0.9720 - val_loss: 0.0706 - val_accuracy: 0.9769 - lr: 3.1250e-05\n",
      "Epoch 51/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0728 - accuracy: 0.9743\n",
      "Epoch 51: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0720 - accuracy: 0.9747 - val_loss: 0.0713 - val_accuracy: 0.9769 - lr: 3.1250e-05\n",
      "Epoch 52/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9707\n",
      "Epoch 52: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0695 - accuracy: 0.9707 - val_loss: 0.0701 - val_accuracy: 0.9769 - lr: 3.1250e-05\n",
      "Epoch 53/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9742\n",
      "Epoch 53: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0731 - accuracy: 0.9742 - val_loss: 0.0700 - val_accuracy: 0.9751 - lr: 1.5625e-05\n",
      "Epoch 54/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.9723\n",
      "Epoch 54: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0683 - accuracy: 0.9725 - val_loss: 0.0772 - val_accuracy: 0.9769 - lr: 1.5625e-05\n",
      "Epoch 55/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9742\n",
      "Epoch 55: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0644 - accuracy: 0.9742 - val_loss: 0.0733 - val_accuracy: 0.9769 - lr: 1.5625e-05\n",
      "Epoch 56/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0655 - accuracy: 0.9706\n",
      "Epoch 56: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0661 - accuracy: 0.9702 - val_loss: 0.0739 - val_accuracy: 0.9769 - lr: 1.5625e-05\n",
      "Epoch 57/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9755\n",
      "Epoch 57: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0651 - accuracy: 0.9751 - val_loss: 0.0712 - val_accuracy: 0.9769 - lr: 1.5625e-05\n",
      "Epoch 58/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9716\n",
      "Epoch 58: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0680 - accuracy: 0.9716 - val_loss: 0.0729 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0711 - accuracy: 0.9739\n",
      "Epoch 59: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0714 - accuracy: 0.9733 - val_loss: 0.0746 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0635 - accuracy: 0.9741\n",
      "Epoch 60: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.0646 - accuracy: 0.9733 - val_loss: 0.0756 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0747 - accuracy: 0.9710\n",
      "Epoch 61: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0756 - accuracy: 0.9711 - val_loss: 0.0736 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9711\n",
      "Epoch 62: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0676 - accuracy: 0.9711 - val_loss: 0.0712 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9733\n",
      "Epoch 63: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0618 - accuracy: 0.9733 - val_loss: 0.0714 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9778\n",
      "Epoch 64: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0572 - accuracy: 0.9778 - val_loss: 0.0718 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9755\n",
      "Epoch 65: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0656 - accuracy: 0.9760 - val_loss: 0.0725 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9719\n",
      "Epoch 66: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0658 - accuracy: 0.9716 - val_loss: 0.0724 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9719\n",
      "Epoch 67: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0675 - accuracy: 0.9720 - val_loss: 0.0719 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9768\n",
      "Epoch 68: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0613 - accuracy: 0.9769 - val_loss: 0.0751 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 0.0612 - accuracy: 0.9762\n",
      "Epoch 69: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0605 - accuracy: 0.9765 - val_loss: 0.0773 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9732\n",
      "Epoch 70: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0631 - accuracy: 0.9733 - val_loss: 0.0744 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0609 - accuracy: 0.9756\n",
      "Epoch 71: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0603 - accuracy: 0.9760 - val_loss: 0.0769 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0664 - accuracy: 0.9733\n",
      "Epoch 72: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0670 - accuracy: 0.9733 - val_loss: 0.0769 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9759\n",
      "Epoch 73: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0635 - accuracy: 0.9756 - val_loss: 0.0793 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9724\n",
      "Epoch 74: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0638 - accuracy: 0.9729 - val_loss: 0.0754 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9733\n",
      "Epoch 75: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0638 - accuracy: 0.9733 - val_loss: 0.0724 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9778\n",
      "Epoch 76: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.0590 - accuracy: 0.9778 - val_loss: 0.0724 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9737\n",
      "Epoch 77: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.0642 - accuracy: 0.9733 - val_loss: 0.0757 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9732\n",
      "Epoch 78: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0623 - accuracy: 0.9733 - val_loss: 0.0746 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0651 - accuracy: 0.9733\n",
      "Epoch 79: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0638 - accuracy: 0.9738 - val_loss: 0.0752 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9746\n",
      "Epoch 80: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0605 - accuracy: 0.9747 - val_loss: 0.0736 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0647 - accuracy: 0.9710\n",
      "Epoch 81: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0638 - accuracy: 0.9716 - val_loss: 0.0744 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9720\n",
      "Epoch 82: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0609 - accuracy: 0.9720 - val_loss: 0.0748 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9765\n",
      "Epoch 83: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0595 - accuracy: 0.9765 - val_loss: 0.0739 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0607 - accuracy: 0.9751\n",
      "Epoch 84: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0607 - accuracy: 0.9751 - val_loss: 0.0753 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9706\n",
      "Epoch 85: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 15ms/step - loss: 0.0671 - accuracy: 0.9711 - val_loss: 0.0756 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9742\n",
      "Epoch 86: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0584 - accuracy: 0.9742 - val_loss: 0.0744 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9733\n",
      "Epoch 87: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0587 - accuracy: 0.9733 - val_loss: 0.0764 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0658 - accuracy: 0.9738\n",
      "Epoch 88: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0647 - accuracy: 0.9738 - val_loss: 0.0777 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9710\n",
      "Epoch 89: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0714 - accuracy: 0.9707 - val_loss: 0.0747 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9769\n",
      "Epoch 90: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0594 - accuracy: 0.9773 - val_loss: 0.0727 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0597 - accuracy: 0.9792\n",
      "Epoch 91: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0593 - accuracy: 0.9791 - val_loss: 0.0733 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0632 - accuracy: 0.9760\n",
      "Epoch 92: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0628 - accuracy: 0.9760 - val_loss: 0.0757 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9774\n",
      "Epoch 93: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0578 - accuracy: 0.9773 - val_loss: 0.0764 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0591 - accuracy: 0.9784\n",
      "Epoch 94: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0623 - accuracy: 0.9769 - val_loss: 0.0786 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0640 - accuracy: 0.9728\n",
      "Epoch 95: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0631 - accuracy: 0.9733 - val_loss: 0.0740 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9759\n",
      "Epoch 96: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0583 - accuracy: 0.9760 - val_loss: 0.0744 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9756\n",
      "Epoch 97: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0587 - accuracy: 0.9756 - val_loss: 0.0739 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.0570 - accuracy: 0.9763\n",
      "Epoch 98: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0576 - accuracy: 0.9760 - val_loss: 0.0757 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.0621 - accuracy: 0.9775\n",
      "Epoch 99: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0611 - accuracy: 0.9782 - val_loss: 0.0734 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9764\n",
      "Epoch 100: val_accuracy did not improve from 0.97869\n",
      "71/71 [==============================] - 1s 14ms/step - loss: 0.0602 - accuracy: 0.9765 - val_loss: 0.0738 - val_accuracy: 0.9769 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f706c13e260>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 23:16:48.433180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31350 MB memory:  -> device: 0, name: CUDA GPU, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2024-07-31 23:16:48.433750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31350 MB memory:  -> device: 1, name: CUDA GPU, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
      "2024-07-31 23:16:48.434243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1883] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31350 MB memory:  -> device: 2, name: CUDA GPU, pci bus id: 0000:86:00.0, compute capability: 7.0\n",
      "2024-07-31 23:16:50.196455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 63ms/step - loss: 0.0795 - accuracy: 0.9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07954917848110199, 0.9660032987594604]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_model = load_model('/tmp/CMS_CNN_1D_CheckPoint.h5')\n",
    "cp_model.evaluate(X_test, y_test, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 1s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = cp_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_single = CLASSES[np.argmax(y_pred, axis = -1)]\n",
    "actual_single = CLASSES[np.argmax(y_test, axis = -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "indices = np.random.choice(range(len(X_test)), n_to_show)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    data = X_test[idx]\n",
    "    ax = fig.add_subplot(1, n_to_show, i+1)\n",
    "    ax.plot(data)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, -0.2, 'pred = ' + str(preds_single[idx]), fontsize=10, ha='center', transform=ax.transAxes) \n",
    "    ax.text(0.5, -0.4, 'act = ' + str(actual_single[idx]), fontsize=10, ha='center', transform=ax.transAxes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9620    0.9728    0.9673       624\n",
      "           1     0.9704    0.9588    0.9646       582\n",
      "\n",
      "    accuracy                         0.9660      1206\n",
      "   macro avg     0.9662    0.9658    0.9659      1206\n",
      "weighted avg     0.9661    0.9660    0.9660      1206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "y_test_classes = tf.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion MatriX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "            Legitimate  Suspicious\n",
      "Legitimate         607          17\n",
      "Suspicious          24         558\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "class_labels = ['Legitimate', 'Suspicious']\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=class_labels, columns=class_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (NGC 24.01 / TensorFlow 2.14) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
